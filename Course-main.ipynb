{
   "cells": [
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<table>\n",
            "  <tr>\n",
            "    <td><p style=\"font-size:45px; color: #55BBD2\">Analysis of light microscopy images in Python</p></td>\n",
            "    <td><img src=\"ressources/lmb_logo.svg\" alt=\"LMB Logo\" width=\"500\" height=\"600\" align=\"right\"></td>\n",
            "  </tr>\n",
            "</table>\n",
            "<table>\n",
            "  <tr>\n",
            "    <td><p style=\"font-size:15px; color: #55BBD2\">Version: February 2024</p></td>\n",
            "  </tr>\n",
            "</table>"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Introduction to the course\n",
            "\n",
            "The course aims to introduce you to the essential knowledge to image processing and analysis of microscopic data in Python. \n",
            "\n",
            "At the end of this course, we hope the course attendees will be able to:\n",
            "- understand and establish an image analysis workflow\n",
            "- run a Python notebook\n",
            "- get familiar with relevant Python packages\n",
            "- load and read microscopic image using the package AICSImageOI\n",
            "- filter the noise of an image\n",
            "- segment cells using watershed segmentation\n",
            "- segment cells using cellpose\n",
            "- extract relevant information from an image using the module regionprops and save the results in an excel sheet\n",
            "- track a particle and trace the trajectories of the particles\n",
            "\n",
            "To tackle each of those topics, we will have three analysis problem. \n",
            "To run the Python notebook, select Kernel and choose the environment 'imaging'.\n",
            "\n",
            "## Downloading data\n",
            "\n",
            "Let us first download some data while looking at how to load a package. A package contains functions or modules that are ready to use.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "from urllib.request import urlopen\n",
            "from io import BytesIO\n",
            "from zipfile import ZipFile\n",
            "from pathlib import Path\n",
            "\n",
            "def download_and_unzip(url, extract_to='.'):\n",
            "    http_response = urlopen(url)\n",
            "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
            "    zipfile.extractall(path=extract_to)\n",
            "\n",
            "url = 'https://cloud.mrc-lmb.cam.ac.uk/s/AegYyMp3ajfaZME/download/data.zip'\n",
            "data_folder = Path('./data')\n",
            "download_and_unzip(url, data_folder)\n",
            "\n",
            "print(f'List of files in the {data_folder} folder')\n",
            "for x in (data_folder).glob('*'):\n",
            "    print(f' {str(x.name)}')\n",
            " "
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# First analysis problem\n",
            "\n",
            "## Measurement in a region of interest (ROI)\n",
            "\n",
            "![title](ressources/data1.png)\n",
            "\n",
            "In this first analysis problem, let say we are interested in what is happening in each channel within the actin region (Channel 1) that surround each nucleus (Channel 3). \n",
            "\n",
            "Our region of interest (ROI) is therefore defined by the region delimited by the actins in Channel 1 that have as a \"seed\" the nuclei in Channel 3. \n",
            "\n",
            "We want to measure:\n",
            "- the maximal intensity in Channel 0 within this ROI,\n",
            "- the median intensity in each of the ROI of Channel 1,  \n",
            "\n",
            "The workflow of the analysis is therefore as follows:\n",
            "\n",
            "<img src=\"ressources/workflow/workflow1.png\" alt=\"drawing\" width=\"800\"/>\n",
            "\n",
            "As an extra exercice after the course, you can also measure the number of dots in Channel 2 in each of the ROI. <br>"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Loading microscopy image data\n",
            "\n",
            "For reading various type of microscopy data we suggest AICSImageIO.\n",
            "\n",
            "[AICSImageIO](https://github.com/AllenCellModeling/aicsimageio) is a Python package that enable an unified and standardized reading of microscopic images in different formats. This package will use Python reader if possible or use Bio-Formats as a backup.\n",
            "\n",
            "[Bio-Formats](http://www.openmicroscopy.org/bio-formats/) is a software tool for reading and writing life science image data using standardized, open formats. At the moment, it can supports 162 formats such as nd2, tifffile, lsm, etc. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "from aicsimageio import AICSImage\n",
            "\n",
            "image = AICSImage(data_folder / 'airyscan-4colors.tif')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The image and its metadata are now loaded into the variable \"image\". Next, we read the image and the metadata. \n",
            "\n",
            "### Metadata\n",
            "\n",
            "Metadata stores information describing the data. It includes the number of channels in the images, the name of each channel and emission wavelength associated with each channel, the voxel size, and many other acquisition parameters.\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "image.data\n",
            "image.metadata\n",
            "image.physical_pixel_sizes\n",
            "image.shape\n",
            "image.dims\n",
            "image.channel_names"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<div class=\"alert alert-success\">\n",
            "\n",
            "#### Exercise \n",
            "\n",
            "In the next cell, get familiar with the contents of the variable \"image\" and the metadata by printing the values stored in it one by one and understand the outputs. \n",
            "    \n",
            "Hint: use the function print()\n",
            "    \n",
            "</div>"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": []
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Image dimension and pixel sizes\n",
            "\n",
            "Next, we will check the different dimensions of our image. To illustrate this, we need to know what a dictionnary is as it is required here."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "image.dims"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "for dim, element in enumerate(image.dims.order):\n",
            "    print(f'Array axis {dim} is {element} with size {image.dims[element][0]}')\n",
            "\n",
            "print(f'The name of the channels are {image.channel_names}')\n",
            "print(f'The pixel size in X is {image.physical_pixel_sizes}')\n",
            "print(f'The pixel size in X is {image.physical_pixel_sizes.X:.4f} microns')\n",
            "print(f'The pixel size in Y is {image.physical_pixel_sizes.Y:.4f} microns')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### What is an image?\n",
            "\n",
            "A digital image is a finite numeric representation of the intensity values. It is composed of picture elements known as pixels. \n",
            "\n",
            "In Python, it can be manipulated as a N-dimensional array using the class numpy.ndarray."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "# import the numpy package used for manipulating images\n",
            "import numpy as np\n",
            "\n",
            "# get the array of pixels intensity as a numpy object\n",
            "data = image.data\n",
            "\n",
            "# Print the type of the array\n",
            "print('The data is a ', type(data)) \n",
            "\n",
            "# Print the dimension of the array\n",
            "print('The array has the following shape', data.shape) \n",
            "\n",
            "# Print the physical pixel sizes and the types\n",
            "pixel = image.physical_pixel_sizes\n",
            "print('The physical pixel sizes are', pixel)\n",
            "print('The pixel type is a', type(pixel)) \n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Let us now display an image. \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "# import matplotlib for loading images\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# Let's display the second channel as a 2D image in a figure\n",
            "plt.imshow(data[0,1,0,:,:], cmap='gray')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<div class=\"alert alert-success\">\n",
            "       \n",
            "#### Exercise \n",
            "\n",
            "Display the second and third channels in the image and change the colormap cmap value to 'hot'.\n",
            "    \n",
            "Hint: use the same function that in the previous cell.\n",
            "</div>"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now let's display all the channels of the image in a subplot using a `for` loop:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "# import matplotlib for displaying images\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# compute the number of channels\n",
            "num_channels = image.shape[1]\n",
            "\n",
            "# Display the image for each channel\n",
            "fig, ax = plt.subplots(1, num_channels, figsize=(16,4))\n",
            "for k  in range(num_channels):    \n",
            "    ax[k].imshow(image.data[0,k,0], cmap='hot')\n",
            "    ax[k].set_axis_off()\n",
            "    ax[k].set_title(image.channel_names[k])    "
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "In the following cell, we crop a part of the image, display it in a figure and overlay the pixel values."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "# Crop and downsample the array by a factor of 5\n",
            "crop = image.data[0, 0, 0, 850:950:5, 900:1000:5] \n",
            "\n",
            "# Create a figure with axes\n",
            "fig, ax = plt.subplots(figsize=(5, 5))\n",
            "\n",
            "# Display the downsampled array in the figure\n",
            "plt.imshow(crop, cmap='gray')\n",
            "\n",
            "# Add the values of the pixel intensity\n",
            "for i in range(crop.shape[0]):\n",
            "    for j in range(crop.shape[1]):\n",
            "        c = 'white' if crop[i, j] < 5 else 'black'\n",
            "        ax.text(j, i, str(int(crop[i, j])), color=c, ha='center', va='center', size=10)\n",
            "ax.axis(\"off\")\n",
            "plt.title('Pixel values');"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<div class=\"alert alert-success\">\n",
            "       \n",
            "#### Exercise \n",
            "    \n",
            "Modify the previous cell to\n",
            "\n",
            "1. select a different region within the image and display an overlay of the image with the pixel values,\n",
            "2. check again the size of the image to be sure of not going out of bounds,\n",
            "3. choose an appropriate downsampling factor,\n",
            "4. adjust the figsize for a proper and visible display if necessary.\n",
            "    \n",
            "</div>"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Image filtering and noise\n",
            "\n",
            "Filtering is a technique for modifying or enhancing an image. We can filter an image to emphasize certain features or remove other features. Noise filtering is an example of image filtering.\n",
            "\n",
            "Noise in microscopic image is unavoidable during image acquisition. There are three main categories of noise: \n",
            "1. Photon noise, also called shot noise: acquired due to the fluctuation of photon emission to the detector. This is the most dominant noise. It is signal dependent and follows a Poisson distribution. \n",
            "2. Dark current or dark noise: accumulated over time due to thermal change as the detector heats up. It is unavoidable but negligible compared to photon noise and is not photon dependent. It follows Poisson distribution. \n",
            "3. Readout noise: digital noise acquired during the electronic quantisation of photons. It follows a Gaussian distribution where the standard deviation is constant and the mean is 0.\n",
            "\n",
            "Dominant noise in fluorescence microscopy follows either a Gaussian distribution or Poisson distribution or both but mostly both. Gaussian noise is additive while Poisson noise is signal dependent. A correct removal of those two different types of noise may therefore differ. Firstly, we will look at how those noise affect the image differently. \n",
            "\n",
            "### Simulating noisy images\n",
            "\n",
            "Here we are going to apply our understanding of what an image is to simulate a new one. <br>\n",
            "We will add Gaussian and Poisson noise into our image to see the difference in the effect. \n",
            "\n",
            "In creating the image, we will use the floor division // operator, which returns the integer value of the quotient. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "\n",
            "# Create a noise free test image\n",
            "n = 100\n",
            "nphotons = 5\n",
            "data = np.zeros((n,n),dtype=float)+0.5\n",
            "data[n//4-10:n//4+10, n//4-10:n//4+10] = nphotons\n",
            "data[n//4-10:n//4+10, 3*n//4-10:3*n//4+10] = nphotons\n",
            "data[3*n//4-10:3*n//4+10, n//4:3*n//4] = nphotons\n",
            "\n",
            "# Add a Gaussian noise\n",
            "mean = 0\n",
            "stddev = 3\n",
            "noisy_gauss_img = data + np.random.normal(mean, stddev, data.shape)\n",
            "\n",
            "# Apply Poisson noise\n",
            "noisy_poisson_img = np.random.poisson(data)\n",
            "\n",
            "fig, axs = plt.subplots(1, 3)\n",
            "axs[0].imshow(data, cmap='gray',vmin=-3,vmax=nphotons+3)\n",
            "axs[0].set_title('Original image')\n",
            "axs[0].set_axis_off()\n",
            "\n",
            "axs[1].imshow(noisy_gauss_img, cmap='gray',vmin=-3,vmax=nphotons+3)\n",
            "axs[1].set_title('With Gaussian noise')\n",
            "axs[1].set_axis_off()\n",
            "\n",
            "axs[2].imshow(noisy_poisson_img, cmap='gray',vmin=-3,vmax=nphotons+3)\n",
            "axs[2].set_title('With Poisson noise')\n",
            "axs[2].set_axis_off()\n",
            "\n",
            "fig.set_tight_layout('tight')\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Noise filtering\n",
            "\n",
            "There are different types of noise reduction such as smoothing filtering or convolution filtering, median filtering, or frequency filtering. \n",
            "\n",
            "As each pixel has a value which represents the image intensity at the spatial position of the pixel, we need to work on each pixel of the image to filter an image.\n",
            "\n",
            "### Gaussian filter\n",
            "A smoothing filter uses the principle of convolution to reduce the noise. A convolution process is similar to drawing. The kernel is the pencil that is used for the drawing. So the sharpness of the drawn image really depends on the width of the point of the pencil. In the case of a Gaussian filtering, the Gaussian distribution is used as a kernel. When used for noise filterning, the noise which should have a lower intensity value than the real signal gets smoothed out because of this convolution process."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "from skimage import filters\n",
            "\n",
            "# Crop the image and select the first channel\n",
            "crop = image.data[0, 0, 0, 850:950, 900:1000]\n",
            "\n",
            "# Gaussian filtering of data with a Gaussian filter of a given sigma\n",
            "sigma = 3\n",
            "gaussian_filtered = filters.gaussian(crop.astype(float), sigma)\n",
            "\n",
            "fig, axs = plt.subplots(1, 2)\n",
            "vmin, vmax = crop.min(), crop.max()\n",
            "\n",
            "axs[0].imshow(crop, vmin = vmin, vmax = vmax)                      # original image\n",
            "axs[0].set_title('Noisy image')\n",
            "axs[0].set_axis_off()\n",
            "\n",
            "axs[1].imshow(gaussian_filtered, vmin = vmin, vmax = vmax)                # Gaussian filtered\n",
            "axs[1].set_title('Gaussian filtered ($\\sigma=$'+str(sigma)+')')\n",
            "axs[1].set_axis_off()\n",
            "\n",
            "plt.tight_layout()\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<div class=\"alert alert-success\">\n",
            "       \n",
            "#### Exercise \n",
            "\n",
            "In the previous cell, change the standard deviation sigma of the Gaussian kernel and observe the change in the Gaussian filtered image.\n",
            "    \n",
            "</div>"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Median filter\n",
            "A different technique for noise filtering is a median filtering. With the median filter, each output pixel is computed as the median value of the input pixel under a chosen window. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "from skimage import filters\n",
            "\n",
            "# define the kernel\n",
            "median_kernel = np.ones((5,5)) \n",
            "\n",
            "# the median kernel is introduced in the second argument of the function \n",
            "median_filtered = filters.median(crop, median_kernel) \n",
            "\n",
            "fig, axs = plt.subplots(1, 2)\n",
            "\n",
            "# compute the min and max of the original image to keep the same display range\n",
            "vmin, vmax = crop.min(), crop.max()\n",
            "\n",
            "axs[0].imshow(crop, vmin = vmin, vmax = vmax)    \n",
            "axs[0].set_title('Noisy image')\n",
            "axs[0].set_axis_off()\n",
            "\n",
            "axs[1].imshow(median_filtered,  vmin = vmin, vmax = vmax)    \n",
            "axs[1].set_title('Median filtered')\n",
            "axs[1].set_axis_off()\n",
            "\n",
            "plt.tight_layout()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {
            "tags": []
         },
         "source": [
            "<div class=\"alert alert-success\">\n",
            "\n",
            "#### Exercise       \n",
            "\n",
            "Change the size of the median kernel and observe the change in the median filtered image.\n",
            "    \n",
            "</div>"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Segmentation\n",
            "\n",
            "Image segmentation is the process of partitioning an image into multiple distinct regions. In semantic segmentation, regions can represent objects of interests against background. They can then be represented as a binary mask. Several type of objects can be further encoded using a label image. Finally, instance segmentation aims at distinguishing different identities of objects of potentially the same class and can also be represented using a label image.\n",
            "\n",
            "To segment, we need to find the masks which represent each region of interest (ROI). This can easily be done by simply thresholding the intensity of the image. By this, the mask is defined such that only intensity (pixel) values greater than the threshold are selected. If we stop at this stage, we may encounter a problem such that some values within the thresholded region may get discarded because the intensity value is lower than the threshold and the ROI won't be filled. In this case, we may need to find the edge or/and fill the mask region. Another issue that we may encounter is that the edges of the regions may overlap so they may be detected or segmented as one. One solution in this case is to use what is called watershed segmentation. This technique helps to detect the flow of the intensity and decide from there whether the regions form only one object or different objects. \n",
            "\n",
            "### Thresholding \n",
            "\n",
            "The simplest way of doing this is to use a manually selected threshold, where an intensity value is defined as a threshold."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "from skimage import filters\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "\n",
            "nuclei = image.data[0,3,0,:,:] # the object we want to segment and set as markers of our final ROI\n",
            "nuclei_flt = filters.gaussian(nuclei.astype(float), 10, preserve_range=True) \n",
            "\n",
            "threshold = 5 # change threshold value here to see what happens to the mask that is being displayed below\n",
            "mask = nuclei_flt >= threshold    \n",
            "plt.imshow(mask, cmap='gray')\n",
            "plt.title('Thresholded image')\n",
            "plt.axis('off')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Lets plot the contour of this mask over the image to judge the quality of the segmentation."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "from skimage import measure\n",
            "\n",
            "contours = measure.find_contours(mask)\n",
            "\n",
            "plt.imshow(nuclei)\n",
            "# For each elements of the contours\n",
            "for contour in contours:\n",
            "    plt.plot(contour[:,1], contour[:,0])\n",
            "\n",
            "plt.axis('off')\n",
            "plt.title('Contours of the segmented image');"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Automated thresholding segmentation \n",
            "\n",
            "If a single threshold is not suitable for a collection of images, a thresholding algorithm can be used. One example of this is Otsu's method [Otsu's method](https://en.wikipedia.org/wiki/Otsu%27s_method). More thresholding algorithms can be found in the [skimage.filters package](https://scikit-image.org/docs/stable/api/skimage.filters.html). "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "from skimage import filters\n",
            "\n",
            "# find the threshold value differentiating the signal from background\n",
            "threshold = filters.threshold_otsu(nuclei_flt) \n",
            "\n",
            "# define the threshold mask and close it\n",
            "nuclei_mask = nuclei_flt > threshold\n",
            "\n",
            "fig, ax = plt.subplots(1,3)\n",
            "\n",
            "ax[0].imshow(nuclei_flt)\n",
            "ax[0].set_title('Thresholded image')\n",
            "ax[0].set_axis_off()\n",
            "\n",
            "ax[1].hist(nuclei.ravel(),log=True)\n",
            "ax[1].plot([threshold, threshold], [0,5e6])\n",
            "ax[1].set_title('Otsu threshold')\n",
            "ax[1].set_aspect(5)\n",
            "\n",
            "ax[2].imshow(nuclei_mask)\n",
            "ax[2].set_title('Thresholded image')\n",
            "ax[2].set_axis_off()\n",
            "\n",
            "plt.tight_layout()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Post processing of binary masks\n",
            "The mask obtained after thresholding can be further processed using morphological operations."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "from skimage import morphology\n",
            "from skimage import segmentation\n",
            "from skimage import measure\n",
            "from skimage import color\n",
            "\n",
            "# remove artifacts connected to image border\n",
            "only_large_objects = morphology.remove_small_objects(nuclei_mask, 10000)\n",
            "cleared = morphology.closing(only_large_objects, morphology.square(35))\n",
            "cleared = segmentation.clear_border(cleared)\n",
            "\n",
            "# label image regions\n",
            "nuclei_label = measure.label(cleared)\n",
            "\n",
            "# label image to rgb \n",
            "nuclei_label_overlay = color.label2rgb(nuclei_label, image=10*nuclei, bg_label=0)\n",
            "\n",
            "fig, ax = plt.subplots(2, 3, figsize=(8,5))\n",
            "\n",
            "ax[0,0].imshow(nuclei_flt)\n",
            "ax[0,0].set_title('Filtered Nuclei channel')\n",
            "ax[0,0].set_axis_off()\n",
            "\n",
            "\n",
            "ax[0,1].imshow(nuclei_mask)\n",
            "ax[0,1].set_title('Otsu threshold')\n",
            "ax[0,1].set_axis_off()\n",
            "\n",
            "ax[0,2].imshow(only_large_objects)\n",
            "ax[0,2].set_title('Large Objects')\n",
            "ax[0,2].set_axis_off()\n",
            "\n",
            "ax[1,0].imshow(cleared)\n",
            "ax[1,0].set_title('Cleared mask')\n",
            "ax[1,0].set_axis_off()\n",
            "\n",
            "ax[1,1].imshow(nuclei_label)\n",
            "ax[1,1].set_title('Label image')\n",
            "ax[1,1].set_axis_off()\n",
            "\n",
            "ax[1,2].imshow(nuclei_label_overlay)\n",
            "ax[1,2].set_title('Mask and image overlay')\n",
            "ax[1,2].set_axis_off()\n",
            "\n",
            "plt.tight_layout()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Watershed-based segmentation\n",
            "The segmentation of the nuclei is pretty straightforward because they are well separated. In some cases however, the objects we want to segment overlap, making it difficult to discern the different regions. In this case, we would need to add extra step into the image segmentation in order to discern the objects. Watershed segmentation is useful for this purpose. It is generally used for separating different objects. Watershed algorithm treats pixels values as a local topography (elevation) so the goal is to follow the flow of the elevation and find the watershed lines from which the edge of each of the regions are deduced. We use this principle to extract our previously defined ROIs: the actin region in Channel-1 having the two nuclei as markers and the seeds of the segmentation. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "actin  = image.data[0,1,0,:,:] #  the channel we want to segment \n",
            "\n",
            "fig, axs = plt.subplots(1,2)\n",
            "axs[0].imshow(actin)\n",
            "axs[0].set_title('Actin channel')\n",
            "axs[0].set_axis_off()\n",
            "\n",
            "axs[1].imshow(nuclei)\n",
            "axs[1].set_title('Nuclei channel')\n",
            "axs[1].set_axis_off()\n",
            "\n",
            "plt.tight_layout()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Mask in the actin channel\n",
            "Create a mask on the actin using thresholding and morphological operations for closing and filling the mask. In the morphological operations, the erosion and dilation operations are used: <br>\n",
            "- Erosion: removes small objects so only the big objects will remain in the image,\n",
            "- Dilation: makes small objects more visible and fills in small holes in objects,\n",
            "- Closing: a successive operation of those two operations dilation followed by an erosion.\n",
            "\n",
            "The dynamic range of the intensity values of the actin image is low and it has a 8 bit depth. In this situation, it is very easy to clip out some intensity range or completely change the intensity range. This may yield to a huge error in the analysis. It is always advised to record an image at higher depth or increase the bit depth of the image. Here we convert the 8-bit image into 32-bit floating point."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "# convert 8-bit into 32-bit floating point\n",
            "print('bit depth: ', actin.dtype)\n",
            "print('(min, max) of the dynamic range of actin =', (np.min(actin), np.max(actin)))\n",
            "\n",
            "actin_float = actin.astype(np.float32)\n",
            "print('bit depth: ', actin_float.dtype)\n",
            "\n",
            "print('(min, max) of the dynamic range of actin_float =', (np.min(actin_float), np.max(actin_float)))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "In the following cell, firstly run the cell. Secondly, replace the input argument actin_float of the filters.gaussian by actin and observe the difference. Thirdly, replace it back to actin_float as the result from actin if of no interest to us."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "from scipy import ndimage as ndi\n",
            "from skimage import filters\n",
            "\n",
            "# smoothed the image using the Gaussian filter with the same sigma as before\n",
            "smooth_actin = filters.gaussian(actin_float, sigma=3)\n",
            "print('(min, max) of the dynamic range of smooth_actin =', (np.min(smooth_actin), np.max(smooth_actin)))\n",
            "\n",
            "# find the optimal threshold\n",
            "threshold = 0.5\n",
            "\n",
            "# define the threshold mask\n",
            "thresh_mask = smooth_actin > threshold\n",
            "\n",
            "# fill holes to form the the actin_mask without performing the closing function\n",
            "actin_mask = ndi.binary_fill_holes(thresh_mask).astype(int)\n",
            "\n",
            "fig, axs = plt.subplots(1,4, figsize=(15, 15))\n",
            "axs[0].imshow(actin)\n",
            "axs[0].set_title('Original image')\n",
            "axs[0].set_axis_off()\n",
            "\n",
            "axs[1].imshow(smooth_actin)\n",
            "axs[1].set_title('Gaussian filter')\n",
            "axs[1].set_axis_off()\n",
            "\n",
            "axs[2].imshow(thresh_mask)\n",
            "axs[2].set_title('Thresholded image')\n",
            "axs[2].set_axis_off()\n",
            "\n",
            "axs[3].imshow(actin_mask)\n",
            "axs[3].set_title('Fill holes')\n",
            "axs[3].set_axis_off()\n",
            "\n",
            "plt.tight_layout()\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Distance transform and watershed\n",
            "The next step will consists of computing the distance transform of the actin mask. Observe how the value of the distance inside the regions change with the spatial coordinates. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "from scipy import ndimage as ndi\n",
            "distance = ndi.distance_transform_edt(actin_mask) \n",
            "\n",
            "fig, ax = plt.subplots(1,3)\n",
            "\n",
            "ax[0].imshow(actin_mask)\n",
            "ax[0].set_title('Mask')\n",
            "ax[0].set_axis_off()\n",
            "\n",
            "im = ax[1].imshow(distance, cmap='jet')\n",
            "ax[1].set_title('Distance transform')\n",
            "ax[1].set_axis_off()\n",
            "\n",
            "# insert the colorbar \n",
            "ax[2].set_position([0.64,0.4,0.025,0.2])\n",
            "fig.colorbar(im, cax=ax[2])\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now we set the nuclei we segmented before as the markers or seeds of the segmentations. We use the markers to label the segmented regions in the actin region. \n",
            "After, we watershed the negative inverse of the distance transform of the actin using the defined markers and within the mask delimited by the actin. The negative inverse of the distance is used in the watershed function instead of the distance because the goal is to have the objects region as valleys not peak.  "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "from skimage import segmentation\n",
            "\n",
            "cell_label = segmentation.watershed(-distance, markers = nuclei_label, mask = actin_mask)\n",
            "\n",
            "fig, ax = plt.subplots(1,4, figsize=(15, 15))\n",
            "\n",
            "# create a RGB composite image to display the two channels (normalize channel in between 0 and 1)\n",
            "rgb = np.stack([actin.astype(float) / actin.max(), \n",
            "                np.zeros(actin.shape), \n",
            "                nuclei.astype(float) / nuclei.max()], axis=2)\n",
            "\n",
            "ax[0].imshow(rgb)\n",
            "ax[0].set_axis_off()\n",
            "ax[0].set_title('Actin and nuclei')\n",
            "\n",
            "ax[1].imshow(distance, cmap='jet')\n",
            "ax[1].set_title('Distance')\n",
            "ax[1].set_axis_off()\n",
            "\n",
            "ax[2].imshow(nuclei_label)\n",
            "ax[2].set_title('Markers (seeds of segmentation)')\n",
            "ax[2].set_axis_off()\n",
            "\n",
            "ax[3].imshow(cell_label)\n",
            "ax[3].set_title('Segmented ROIs')\n",
            "ax[3].set_axis_off()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<div class=\"alert alert-success\">\n",
            "\n",
            "#### Exercise       \n",
            "\n",
            "Copy the cell above and change the code to perform a watershed segmentation using the actin channel as a landscape image. Observe the results.\n",
            "   \n",
            "</div>\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Region properties\n",
            "\n",
            "To measure information within the ROI masks that we have previously segmented, we use the regionprops function. It allows us to define a list of geometrical and intensity properties which can be measured in the images. \n",
            "\n",
            "This is a selection of useful properties supported by regionprops:\n",
            "  - area\n",
            "  - bbox\n",
            "  - bbox_area\n",
            "  - centroid\n",
            "  - coords\n",
            "  - equivalent_diameter\n",
            "  - label\n",
            "  - major_axis_length\n",
            "  - max_intensity\n",
            "  - mean_intensity\n",
            "  - min_intensity\n",
            "  - minor_axis_length\n",
            "  - slice\n",
            "  - solidity\n",
            "  - eccentricity\n",
            "  - orientation\n",
            "  - perimeter\n",
            "\n",
            "### Maximum Intensity\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "from skimage import measure\n",
            "\n",
            "\n",
            "signal = image.data[0,0,:,:].squeeze()\n",
            "signal_table = measure.regionprops_table(cell_label, signal, properties=('label', 'intensity_max'))\n",
            "results = pd.DataFrame(signal_table)\n",
            "\n",
            "results"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "from skimage import measure\n",
            "\n",
            "signal = image.data[0,0,:,:].squeeze()\n",
            "\n",
            "signal_props = measure.regionprops(cell_label, signal)\n",
            "actin_props = measure.regionprops(cell_label, actin)\n",
            "\n",
            "# Get the list of labels from the regions properties\n",
            "labels = [p.label for p in signal_props]\n",
            "\n",
            "# Get the maximum intensity for the properties\n",
            "signal_max_intensity = [p.intensity_max for p in signal_props]\n",
            "actin_max_intensity = [p.intensity_max for p in actin_props]\n",
            "\n",
            "# Gather the results in a data frame (table)\n",
            "results = pd.DataFrame({\n",
            "    'Region label': labels, \n",
            "    'Signal [ch0] maximum intensity': signal_max_intensity,\n",
            "    'Actin [ch1] maximum intensity': actin_max_intensity})\n",
            "\n",
            "results\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<div class=\"alert alert-success\">\n",
            "\n",
            "#### Exercise \n",
            "\n",
            "Use one of the methods above to measure the perimeter of the nuclei. Store the results in a variable called \"perimeter_nuclei\".\n",
            "</div>"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Defining a custom measurement\n",
            "\n",
            "The information about the median is currently not included in the regionprops function. Here we show how can we extract the intensity median for each ROI using a custom function. The median of a list is defined as the middle element of the list when the list is sorted. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "# Define a custom function to measure median intensity\n",
            "def median_intensity(mask, intensity):\n",
            "    '''Median intenisty in a mask\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    mask : binary image\n",
            "    intensity: intensity  image\n",
            "\n",
            "    Returns\n",
            "    -------\n",
            "    The median intensity in the mask\n",
            "    '''\n",
            "    return np.median(intensity[mask])\n",
            "\n",
            "actin_props = measure.regionprops_table(cell_label, actin, \n",
            "                                 properties = ('label','mean_intensity'), \n",
            "                                 extra_properties = (median_intensity,))\n",
            "\n",
            "results = pd.DataFrame(actin_props)\n",
            "\n",
            "results\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Save result\n",
            "Export the result stored in the panda dataframe into a csv file"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "results.to_csv('mean_median_results.csv')"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<div class=\"alert alert-success\">\n",
            "\n",
            "#### Extra exercise \n",
            "\n",
            "At the end of this course, you can use the techniques described in this Python notebook and find the number of particles in Channel 2 of the data airyscan-4colors.tif within the labelled regions that we segmented above. \n",
            "\n",
            "Hint: <br>\n",
            "1- use the blob detection used in the third analysis problem to find the position of the particles <br>\n",
            "2- generate the locations of all the blobs <br>\n",
            "3- measure the regionprops of the cell mask <br>\n",
            "4- count how many blobs coordinates are there per segmented cell\n",
            "</div>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Second analysis problem\n",
            "\n",
            "## Colocalization problem\n",
            "\n",
            "![title](ressources/data2.png)\n",
            "\n",
            "In this second analysis problem, we will focus on a different problem. Our data consists of an image with four channels. <br>\n",
            "- Channel 0: Golgi marker,\n",
            "- Channel 1: cargo protein,\n",
            "- Channel 2: mitochondria,\n",
            "- Channel 3: the nuclei of the cell. \n",
            "\n",
            "With a particular treatement, the cargo moves to the mitochondria. Without the mentioned treatement, the cargo goes to the Golgi. Our problem then consists of quantifying the relocation of the cargo to the mitochondria and its relocation to the Golgi. We also want to understand how the high (transfected) or low presence (not transfected) of the mitochondria in the cell affect this relocation of the cargo. Hence, the workflow of our image analysis is illustrated as follows : <br>\n",
            "\n",
            "<img src=\"ressources/workflow/workflow2.png\" alt=\"drawing\" width=\"800\"/>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Load data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from aicsimageio import AICSImage, readers\n",
            "\n",
            "col_image = AICSImage(data_folder/'Zeiss1344.lsm', reader=readers.tiff_reader.TiffReader)\n",
            "col_data = col_image.data\n",
            "col_data_shape = col_data.shape"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Segment the cell using cellpose\n",
            "\n",
            "Here we present a different technique for segmenting the cell. It uses a pretrained neural network for segmenting the image using two channels. <br> [Cellpose](https://cellpose.readthedocs.io/en/latest/) has been pretrained to detect cells or nuclei on a number of biological images."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "from skimage import segmentation\n",
            "from cellpose import models\n",
            "\n",
            "# Get the pretrained model\n",
            "model = models.Cellpose(gpu=False, model_type='cyto2')\n",
            "\n",
            "# Evaluate the model\n",
            "mask, flows, styles, diams = model.eval(col_data, channels=[2,3], diameter=400, cellprob_threshold=1.0)\n",
            "\n",
            "# Display the results\n",
            "fig, ax = plt.subplots(1,7, figsize=(15,5))\n",
            "for k in range(4):\n",
            "    ax[k].imshow(col_data[:,k,:,:,:].squeeze())\n",
            "    ax[k].set_axis_off()\n",
            "    ax[k].set_title('Ch0'+str(k))\n",
            "ax[4].imshow(mask)\n",
            "ax[4].set_axis_off()\n",
            "ax[4].set_title('Labels')\n",
            "ax[5].imshow(flows[0])\n",
            "ax[5].set_axis_off()\n",
            "ax[5].set_title('Flows')\n",
            "ax[6].imshow(flows[2])\n",
            "ax[6].set_axis_off()\n",
            "ax[6].set_title('Probability')\n",
            "fig.tight_layout()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Regions properties"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from skimage import measure\n",
            "\n",
            "props = measure.regionprops(mask)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Using an interactive display\n",
            "We will use the module 'interact' to view each labelled segmented region and observe if the segmentation was done as we expected it "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from ipywidgets import interact\n",
            "import numpy as np\n",
            "\n",
            "def displayf(label):\n",
            "    p = props[label]\n",
            "    single_mask = np.zeros(col_data_shape[-2:])\n",
            "    single_mask[p.coords[:,0], p.coords[:,1]] = 1\n",
            "    \n",
            "    plt.imshow(col_data[:,2,:,:].squeeze())                          # mitochondria\n",
            "    plt.imshow(col_data[:,3,:,:].squeeze(), alpha=0.5 , cmap='grey') # nuclei\n",
            "\n",
            "    plt.imshow(single_mask, alpha=0.2)\n",
            "\n",
            "    return \n",
            "\n",
            "interact(displayf, label=(0, len(props)-1, 1)) # min, max, step\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<div class=\"alert alert-success\">\n",
            "\n",
            "#### Exercise       \n",
            "\n",
            "The cell segmentation was obtained after setting the parameters: particle size to 400 pixels and cellprob_threshold = 1.0. <br>\n",
            "Observe the segmented regions change if you change the particle size or the cellprob_threshold.    \n",
            "</div>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Measure the colocalization coefficient \n",
            "\n",
            "The Pearson's correlation coefficient (PCC) is widely used to quantify the colocalization of one object $A$ into another object $B$. It can be calculated using the following formula [1]:\n",
            "\n",
            "$$\\text{PCC} = \\frac{\\sum_i\\left( A_i - A_{aver} \\right) \\cdot \\left( B_i - B_{aver} \\right)}{\\sqrt{\\left[ \\sum_i \\left( A_i - A_{aver} \\right)^2 \\cdot \\sum_i \\left( B_i - B_{aver} \\right)^2 \\right]}}$$\n",
            "\n",
            "$A$: cargo in this problem with $A_i$ being the intensity at voxel $i$ and $A_{aver}$ the average value intensity \\\n",
            "$B$: organelles (mitochondria or Golgi) with $B_i$ being the intensity at voxel $i$ and $B_{aver}$ the average value intensity\n",
            "\n",
            "PCC values range from -1 to 1. We will use the function pearsonr from the scipy.stats module to compute it. We will calculate the PCC between the organelles, mitochondria or Golgi, and the cargo to quantify how much cargo relocates to each corresponding organelle for a given treatement done on the cell. We will observe how different the PCC values are in high presence (transfected) and low presence (not transfected) of mitochondria in each cell respectively. <br>\n",
            "\n",
            "We will apply the following notations:\n",
            "- PCC1: PCC between the cargo and Golgi \n",
            "- PCC2: PCC between the cargo and the mitochondria\n",
            "\n",
            "Source [1]: Manders, E.M., Verbeek, F.J. and Aten, J.A., 1993. Measurement of co‐localization of objects in dual‐colour confocal images. Journal of microscopy, 169(3), pp.375-382."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import scipy.stats\n",
            "from skimage import measure\n",
            "import pandas as pd\n",
            "\n",
            "ColCoeff = []\n",
            "intensity_threshold = 1e6\n",
            "\n",
            "for p in props:\n",
            "    ch00 = col_data[:,0,:,:].squeeze()[p.coords[:,0], p.coords[:,1]] # Golgi marker\n",
            "    ch01 = col_data[:,1,:,:].squeeze()[p.coords[:,0], p.coords[:,1]] # Cargo protein\n",
            "    ch02 = col_data[:,2,:,:].squeeze()[p.coords[:,0], p.coords[:,1]] # Mitochondria\n",
            "    sum_mitochondria = ch02.sum()\n",
            "    if sum_mitochondria >= intensity_threshold:\n",
            "        c = 'Transfected'\n",
            "    else:\n",
            "        c = 'Not transfected'\n",
            "\n",
            "    \n",
            "    [r1, pv] = scipy.stats.pearsonr(ch00, ch01)\n",
            "    [r2, pv] = scipy.stats.pearsonr(ch01, ch02)\n",
            "    ColCoeff.append({'label':p.label,'PCC1':r1, 'PCC2':r2, 'Area':p.area, 'Integrated intensity':sum_mitochondria, 'Condition':c})\n",
            "\n",
            "ColCoeff = pd.DataFrame.from_records(ColCoeff)\n",
            "ColCoeff"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Statistical analysis\n",
            "\n",
            "We firstly analyse the data which corresponds to the relocation of the cargo to the Golgi. The relocation of the cargo to the mitochondria will be done as an exercise.\n",
            "\n",
            "### p-value\n",
            "\n",
            "The p-value is calculated from our statistical test. It describes how likely we are to have found the set of observations we have if there is no relationship between our conditions of interest (amount of mitochondria in each cell)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "grp1 = ColCoeff[ColCoeff['Condition']=='Transfected']\n",
            "grp2 = ColCoeff[ColCoeff['Condition']=='Not transfected']\n",
            "pvalue = scipy.stats.mannwhitneyu(grp1['PCC1'],grp2['PCC1'])\n",
            "\n",
            "print(pvalue)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Display of the result\n",
            "Display the PCC values, annotate the p-value and save the result into pdf file"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import seaborn as sns\n",
            "from statannotations.Annotator import Annotator\n",
            "\n",
            "conditions_list = ['Not transfected', 'Transfected']\n",
            "\n",
            "plotting_parameters = {\n",
            "    'data':    ColCoeff,\n",
            "    'x':       'Condition',\n",
            "    'y':       'PCC1',\n",
            "    'order':   conditions_list,\n",
            "}\n",
            "\n",
            "ax = sns.violinplot(**plotting_parameters, color=\"0.9\")\n",
            "sns.stripplot(**plotting_parameters, jitter=True, size=2)\n",
            "\n",
            "annotator = Annotator(ax, tuple([conditions_list]), **plotting_parameters)\n",
            "annotator.set_pvalues([pvalue.pvalue])\n",
            "annotator.configure(loc='outside')\n",
            "annotator.annotate()\n",
            "\n",
            "ax.set_title('Relocation of the \\nCargo to the Golgi', y=1.0, pad=-10, c='red', horizontalalignment='center')\n",
            "\n",
            "plt.savefig('PCC_Cargo_to_Golgi.pdf', format=\"pdf\", bbox_inches=\"tight\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<div class=\"alert alert-success\">\n",
            "\n",
            "#### Exercise       \n",
            "\n",
            "Display the result of the statistical analysis which corresponds to the relocation of cargo to the mitochondria. <br>\n",
            "Make sure the title of the figure and the name under which the figure will be saved are updated.\n",
            "   \n",
            "</div>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Third analysis problem\n",
            "\n",
            "## Particle tracking\n",
            "There are different methods we can use to track particles. Here we will use a spot detection, called blob detection. This technique aims at detecting regions in a digital image that differ in properties, such as brightness or color, compared to surrounding regions. The Difference of Gaussian (DoG) method is one technique to detect blobs. For each blob found (a spot in our case), the method returns its coordinates position and the standard deviation of the Gaussian kernel that detected the spot. \n",
            "\n",
            "The workflow of our image analysis is illustrated as follows : <br>\n",
            "\n",
            "<img src=\"ressources/workflow/workflow3.png\" alt=\"drawing\" width=\"800\"/>\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from aicsimageio import AICSImage\n",
            "\n",
            "particles = AICSImage(data_folder / '181228_CDX2_9s_c48_n073.tif')\n",
            "particles_data = particles.data\n",
            "sz = particles_data.shape\n",
            "print(sz)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Interactive display\n",
            "Now we are going to display the particles images, consisting of 105 frames, using the 'interact' module."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from ipywidgets import interact\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "def displayf(t):\n",
            "    myimg = np.squeeze(particles_data[t])\n",
            "\n",
            "    return plt.imshow(myimg)\n",
            "\n",
            "interact(displayf, t=(0, sz[0]-1, 1)) # min, max, step"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Detect each particle (spot) using blob_dog from the feature module\n",
            "\n",
            "The blob_dog spot detection has few parameters that we need to care for in order to filter out what we consider particles and which ones are not. There are importants parameters that we will conder here: threshold, min_sigma and max_sigma. \n",
            "\n",
            "- threshold: The absolute lower bound for scale space maxima. Local maxima smaller than threshold are ignored. Reduce this to detect blobs with lower intensities. <br>\n",
            "- min_sigma: The minimum standard deviation for Gaussian kernel.\n",
            "- max_sigma: The maximum standard deviation for Gaussian kernel.\n",
            "\n",
            "Further reading: https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.blob_dog\n",
            "\n",
            "We will save the positions of the detected particles in a panda dataframe."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from skimage import feature \n",
            "import pandas as pd\n",
            "\n",
            "particles_positions = []\n",
            "\n",
            "for t in range(sz[0]):    \n",
            "    coordinates = feature.blob_dog(particles_data[t].squeeze(), min_sigma=1, max_sigma=5, threshold=0.005)    \n",
            "    particles_positions.append(pd.DataFrame({'x': coordinates[:,1], 'y': coordinates[:,0], 'w': coordinates[:,2], 'frame':t}))\n",
            "\n",
            "particles_positions = pd.concat(particles_positions)\n",
            "    \n",
            "particles_positions"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Overlay the particles (yellow crosses) positions on top of the images of the particles"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from ipywidgets import interact\n",
            "\n",
            "def displayPos(t):\n",
            "    myimg = np.squeeze(particles_data[t])\n",
            "    coordinates = particles_positions.loc[particles_positions['frame']==t]\n",
            "\n",
            "    plt.imshow(myimg)\n",
            "    return plt.plot(coordinates['x'], coordinates['y'], '+', color='yellow', linewidth=0.01)   \n",
            "\n",
            "interact(displayPos, t=(0, sz[0]-1, 1)) # min, max, step"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<div class=\"alert alert-success\">\n",
            "\n",
            "#### Exercise       \n",
            "\n",
            "The results we have here were optimized using threshold = 0.005, min_sigma = 1, and max_sigma = 5. You may find a better setting than these. <br> \n",
            "So go back to the spot detection cell and play with the values of those three parameters and observe how the results are changing accordingly.\n",
            "\n",
            "   \n",
            "</div>\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Particles trajectories\n",
            "We are now going to trace the trajectories of the particles by linking the coordinates positions. <br> \n",
            "For this, we will use a Python package named 'trackpy'.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import trackpy as tp\n",
            "\n",
            "farthest_distance = 8 # farthest distance a particle can travel, larger value slows computation time\n",
            "track_linked = tp.link(particles_positions, farthest_distance, memory=5) # value of memory corresponds to the number of frames that is being memorized, track_linked is a pandas dataframe\n",
            "nb_particles = track_linked['particle'].unique().max() + 1\n",
            "print('There are', nb_particles, 'particles')\n",
            "\n",
            "plt.figure()\n",
            "tp.plot_traj(track_linked)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<div class=\"alert alert-success\">\n",
            "\n",
            "#### Exercise       \n",
            "\n",
            "Try to understand the code line which generates the number of particles. <br>\n",
            "What do you think will happen to this number of particles if you change the allowed farthest distance a particle can travel? <br>\n",
            "\n",
            "(Here we have farthest_distance = 8 initially)\n",
            "   \n",
            "</div>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Additional constraints\n",
            "\n",
            "But maybe we eventually want to remove particles that are only appearing in few frames?\n",
            "\n",
            "Let say we won't consider the trajectories of particles that are appearing in less than 10 frames."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "\n",
            "l = track_linked['particle'].unique() \n",
            "allowed_track_length = 10\n",
            "particles_id = [k for k in l if len(track_linked.loc[track_linked['particle']==k])>=allowed_track_length]\n",
            "\n",
            "selected_tracks = pd.DataFrame([])\n",
            "\n",
            "for t in range(len(particles_id)):\n",
            "    coordinates = track_linked.loc[track_linked['particle']==particles_id[t]]\n",
            "    selected_tracks = pd.concat([selected_tracks, coordinates])\n",
            "\n",
            "selected_tracks"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Display the selected tracks over the images of the particles to see how well we did "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def displayTrack(t):\n",
            "    myimg = np.squeeze(particles_data[t])\n",
            "\n",
            "    plt.imshow(myimg)\n",
            "    return tp.plot_traj(selected_tracks)\n",
            "\n",
            "interact(displayTrack, t=(0, sz[0]-1, 1)) # min, max, step"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<div class=\"alert alert-success\">\n",
            "\n",
            "#### Exercise       \n",
            "\n",
            "Change the allowed_track_length and observe the number of trajectories displayed in the overlay display.\n",
            "   \n",
            "</div>"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Exporting tabular results\n",
            "\n",
            "To save results in a csv file use the export function from pandas. <br>\n",
            "The excelsheet will be save in your current directory. To check where is that directory, run the command pwd in a new cell and you will see the csv file be saved in that directory.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pwd"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "selected_tracks.to_csv('tracking_results.csv')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Some more helpful tips\n",
            "\n",
            "- scientific images are data, they can be compromised by inappropriate manipulations\n",
            "- take good images:\n",
            "    - don't oversaturate your data\n",
            "    - use the full dynamic range when taking your images\n",
            "- do not segment on the data you are measuring, use a housekeeping channel\n",
            "- images that are compared to each other need to be processed and acquired in the same manner\n",
            "\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "mrcimaging",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.2"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}
