{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td><p style=\"font-size:45px; color: #55BBD2\">Analysis of light microscopy images in Python</p></td>\n",
    "    <td><img src=\"ressources/lmb_logo.svg\" alt=\"LMB Logo\" width=\"500\" height=\"600\" align=\"right\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><p style=\"font-size:15px; color: #55BBD2\">Version: June 2023</p></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the course\n",
    "\n",
    "The course aims to introduce you to the essential knowledge to image processing and analysis of microscopic data in Python. \n",
    "\n",
    "At the end of this course, we hope the course attendees will be able to:\n",
    "- understand and establish an image analysis workflow\n",
    "- run a Python notebook\n",
    "- get familiar with relevant Python packages\n",
    "- load and read microscopic image using the package AICSImageOI\n",
    "- filter the noise of an image\n",
    "- extract relevant information from an image using the module regionprops and save the results in an excel sheet\n",
    "\n",
    "To run the Python notebook, select Kernel and choose the environment 'imaging'.\n",
    "\n",
    "## Documentation for important python packages\n",
    "\n",
    "- [Scikit Image](https://scikit-image.org/docs/stable/api/api.html)\n",
    "- [Numpy](https://numpy.org/doc/stable/reference/index.html#reference)\n",
    "- [Matplotlib](https://matplotlib.org/stable/api/index.html)\n",
    "- [Cellpose](https://cellpose.readthedocs.io/en/latest/api.html)\n",
    "- [AicsImageIo](https://pypi.org/project/aicsimageio/3.2.1/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working in Python, many functions or modules have already been developped and stored in what is called 'package' or 'library'. \n",
    "\n",
    "A library is a collection of packages while a package is a collection of modules. \n",
    "\n",
    "Let us first download some data while looking at how to load a package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import os \n",
    "\n",
    "def download_and_unzip(url, extract_to='.'):\n",
    "    http_response = urlopen(url)\n",
    "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
    "    zipfile.extractall(path=extract_to)\n",
    "\n",
    "url = 'https://cloud.mrc-lmb.cam.ac.uk/s/apiQPm28YLRWXeo/download/data.zip'\n",
    "data_folder = os.path.expanduser('~/Python-course')\n",
    "download_and_unzip(url, data_folder)\n",
    "\n",
    "from pathlib import Path\n",
    "print(f'List of files in the {data_folder}data folder')\n",
    "for x in (Path(data_folder) / 'data').glob('*'):\n",
    "    print(f' {str(x.name)}')\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis problem\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](ressources/data.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let say we are interested in what is happening in each channel within the actin region (Channel 1) that surround each nucleus (Channel 3). \n",
    "\n",
    "Our region of interest (ROI) is therefore defined by the region delimited by the actins in Channel 1 that have as a \"seed\" the nuclei in Channel 3. \n",
    "\n",
    "We want to measure:\n",
    "- the maximal intensity in Channel 0 within this ROI,\n",
    "- the median intensity in each of the ROI of Channel 1,  \n",
    "- number of dots in Channel 2 in each of the ROI,\n",
    "\n",
    "That is enough problem for now so let's get into it :)\n",
    "\n",
    "To solve the problem and achieve the statistic analysis, we generally need to do have an workflow which generally consists of the following steps:\n",
    "\n",
    "    Step 1: load and read the image and metadata if necessary\n",
    "    Step 2: pre-process the image by filtering it if necessary  \n",
    "    Step 3: find the region of interest by segmentation \n",
    "    Step 4: extract the desired information (i.e. measure image properties) in the ROI: mean, max, min, sum, median, mode, ...\n",
    "    Step 5: display/plot results\n",
    "    Step 6: save the result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading microscopy image data\n",
    "\n",
    "- explain bioformats\n",
    "- what are the different packages for loading a microscopic image?\n",
    "- we use aicsimageio - Why? Depending on your image format you can use a different package\n",
    "\n",
    "[Bio-Formats](http://www.openmicroscopy.org/bio-formats/) is a software tool for reading and writing life science image data using standardized, open formats. Bio-Formats can supports 162 formats at the moment and the list of what can be supported can be found [here](https://bio-formats.readthedocs.io/en/stable/supported-formats.html). [AICSImageIO](https://github.com/AllenCellModeling/aicsimageio) is a Python package that enable an unified and standardized reading of microscopic images in different formats. In case the package runs into problems while reading the image, it is adviced to directly use the format reader listed [here](https://bio-formats.readthedocs.io/en/stable/supported-formats.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scyjava import config\n",
    "from aicsimageio import AICSImage\n",
    "\n",
    "image = AICSImage(data_folder+'/data/airyscan-4colors.tif')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image and its metadata is now loaded into the variable \"image\". Next, we read the image and the metadata. \n",
    "\n",
    "- what is metadata?\n",
    "\n",
    "Metadata stores information that describes a data. It includes the number of channels in the images, the name of each channel and emission wavelength associated with each channel, the voxel size and physical size of the image, and many other acquisition parameters.\n",
    "\n",
    "Exercise: in the next cell, get familiar with the contents of the variable \"image\" and the metadata by printing the values stored in it one by one and understand the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.data\n",
    "image.metadata\n",
    "image.physical_pixel_sizes\n",
    "image.shape\n",
    "image.dims\n",
    "image.channel_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- image dimension and pixel sizes\n",
    "\n",
    "Next, we will check the different dimensions of our image. To illustrate this, we need to know what a dictionnary is as it is required here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim, element in enumerate(image.dims.order):\n",
    "    print(f'Array axis {dim} is {element} with size {image.dims[element][0]}')\n",
    "\n",
    "print(f'The name of the channels are {image.channel_names}')\n",
    "print(f'The pixel size in X is {image.physical_pixel_sizes}')\n",
    "# print(f'The pixel size in X is {image.physical_pixel_sizes.X:.4f} microns')\n",
    "# print(f'The pixel size in Y is {image.physical_pixel_sizes.Y:.4f} microns')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what is an image?\n",
    "\n",
    "A digital image is a finite numeric representation of the intensity values. It is composed of picture elements known as pixels. \n",
    "\n",
    "In Python, it can be read as a N-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the numpy package used for manipulating images\n",
    "import numpy as np\n",
    "\n",
    "# get the array of pixels intensity as a numpy object\n",
    "data = image.data\n",
    "\n",
    "# Print the type of the array\n",
    "print('The data is a ', type(data)) \n",
    "\n",
    "# Print the dimension of the array\n",
    "print('The array has the following shape', data.shape) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now display an image. \n",
    "\n",
    "Exercise: display the second and third channels in the image and change the colormap cmap value to 'hot'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib for loading images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's display a 2D image in a figure\n",
    "plt.imshow(data[0,1,0,:,:], cmap='gray');\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display all the channels of the image in a subplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib for displaying images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# compute the number of channels\n",
    "num_channels = image.shape[1]\n",
    "\n",
    "# Display the image for each channel\n",
    "fig, ax = plt.subplots(1, num_channels, figsize=(16,4))\n",
    "for k  in range(num_channels):    \n",
    "    ax[k].imshow(image.data[0,k,0], cmap='hot')\n",
    "    ax[k].set_axis_off()\n",
    "    ax[k].set_title(image.channel_names[k])    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image is a multi-dimensional array of pixels.\n",
    "\n",
    "Exercise: \n",
    "\n",
    "Select a different region within the image and display an overlay of the image with the pixel values. <br>\n",
    "Check again the size of the image to be sure of not going out of bounds. <br>\n",
    "Choose an appropriate downsampling factor. <br>\n",
    "Adjust the figsize for a proper and visible display if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop and downsample the array by a factor of 5\n",
    "crop = image.data[0, 0, 0, 850:950:5, 900:1000:5] #850\n",
    "\n",
    "# Create a figure with axes\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "# Display the downsampled array in the figure\n",
    "plt.imshow(crop, cmap='gray')\n",
    "\n",
    "# Add the values of the pixel intensity\n",
    "for i in range(crop.shape[0]):\n",
    "    for j in range(crop.shape[1]):\n",
    "        c = 'white' if crop[i, j] < 5 else 'black'\n",
    "        ax.text(j, i, str(int(crop[i, j])), color=c, ha='center', va='center', size=10)\n",
    "ax.axis(\"off\")\n",
    "plt.title('Pixel values');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Filtering and Noise\n",
    "\n",
    "Filtering is a technique for modifying or enhancing an image. We can filter an image to emphasize certain features or remove other features. Noise filtering is an example of image filtering.\n",
    "\n",
    "Noise in microscopic image is unavoidable during image acquisition. There are three main categories of noise: \n",
    "1. Photon noise, also called fluorescence noise or shot noise: acquired due to the fluctuation of photon emission to the detector. This is the most dominant noise. It is signal dependent and follows a Poisson distribution. \n",
    "2. Electronic noise, also called dark current or dark noise: accumulated over time due to thermal change as the detector heats up. It is unavoidable but negligible compared to photon noise and is not photon dependent. It follows Poisson distribution. \n",
    "3. Readout noise: digital noise acquired during the electronic quantisation of photons. It follows a Gaussian distribution where the standard deviation is constant and the mean is 0.\n",
    "\n",
    "Dominant noise in fluorescence microscopy follows either a Gaussian distribution or Poisson distribution or both but mostly both. Gaussian noise is additive while Poisson noise is signal dependent. A correct removal of those two different types of noise may therefore differ. Firstly, we will look at how those noise affect the image differently. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating noisy images\n",
    "\n",
    "Here we are going to apply our understanding of what an image is to simulate a new one. <br>\n",
    "We will add Gaussian and Poisson noise into our image to see the difference in the effect. \n",
    "\n",
    "In creating the image, we will use the floor division // operator, which returns the integer value of the quotient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a noise free test image\n",
    "n = 100\n",
    "nphotons = 5\n",
    "data = np.zeros((n,n),dtype=float)+0.5\n",
    "data[n//4-10:n//4+10, n//4-10:n//4+10] = nphotons\n",
    "data[n//4-10:n//4+10, 3*n//4-10:3*n//4+10] = nphotons\n",
    "data[3*n//4-10:3*n//4+10, n//4:3*n//4] = nphotons\n",
    "\n",
    "# Add a Gaussian noise\n",
    "mean = 0\n",
    "stddev = 2\n",
    "noisy_gauss_img = data + np.random.normal(mean, stddev, data.shape)\n",
    "\n",
    "# Apply Poisson noise\n",
    "noisy_poisson_img = np.random.poisson(data)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "axs[0].imshow(data, cmap='gray',vmin=-3,vmax=nphotons+3)\n",
    "axs[0].set_title('Original image')\n",
    "axs[0].set_axis_off()\n",
    "\n",
    "axs[1].imshow(noisy_gauss_img, cmap='gray',vmin=-3,vmax=nphotons+3)\n",
    "axs[1].set_title('With Gaussian noise')\n",
    "axs[1].set_axis_off()\n",
    "\n",
    "axs[2].imshow(noisy_poisson_img, cmap='gray',vmin=-3,vmax=nphotons+3)\n",
    "axs[2].set_title('With Poisson noise')\n",
    "axs[2].set_axis_off()\n",
    "\n",
    "fig.set_tight_layout('tight')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise filtering\n",
    "\n",
    "There are different types of noise reduction such as smoothing filtering or convolution filtering, median filtering, or frequency filtering. \n",
    "\n",
    "As each pixel has a value which represents the image intensity at the spatial position of the pixel, we need to work on each pixel of the image to filter an image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian filter\n",
    "A smoothing filter uses the principle of convolution to reduce the noise. A convolution process is similar to drawing. The kernel is the pencil that is used for the drawing. So the sharpness of the drawn image really depends on the width of the point of the pencil. In the case of a Gaussian filtering, the Gaussian distribution is used as a kernel. When used for noise filterning, the noise which should have a lower intensity value than the real signal gets smoothed out because of this convolution process. \n",
    "\n",
    "Exercise: change the standard deviation sigma of the Gaussian kernel and observe the change in the Gaussian filtered image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "# Crop the image and select the first channel\n",
    "crop = image.data[0, 0, 0, 850:950, 900:1000]\n",
    "\n",
    "# Gaussian filtering of data with a Gaussian filter of a given sigma\n",
    "sigma = 3\n",
    "gaussian_filtered = filters.gaussian(crop.astype(float), sigma)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "vmin, vmax = crop.min(), crop.max()\n",
    "\n",
    "axs[0].imshow(crop, vmin = vmin, vmax = vmax)                      # original image\n",
    "axs[0].set_title('Noisy image')\n",
    "axs[0].set_axis_off()\n",
    "\n",
    "axs[1].imshow(gaussian_filtered,vmin=vmin,vmax=vmax)                # with addition Gaussian noise\n",
    "axs[1].set_title('Gaussian filtered ($\\sigma=$'+str(sigma)+')')\n",
    "axs[1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median filter\n",
    "A different technique for noise filtering is a median filtering. With the median filter, each output pixel is computed as the median value of the input pixel under a chosen window. \n",
    "\n",
    "Exercise: change the size of the median kernel and observe the change in the Median filtered image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the kernel\n",
    "kernel_size = [5, 5]\n",
    "median_kernel = np.ones((kernel_size[0], kernel_size[1])) \n",
    "\n",
    "# the median kernel is introduced in the second argument of the function \n",
    "median_filtered = filters.median(crop, median_kernel) \n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "# compute the min and max of the original image to keep the same display range\n",
    "vmin, vmax = crop.min(), crop.max()\n",
    "\n",
    "axs[0].imshow(crop, vmin = vmin, vmax = vmax)    \n",
    "axs[0].set_title('Noisy image')\n",
    "axs[0].set_axis_off()\n",
    "\n",
    "axs[1].imshow(median_filtered,  vmin = vmin, vmax = vmax)    \n",
    "axs[1].set_title('Median filtered (' + str(kernel_size[0]) + 'x' + str(kernel_size[1]) + ')')\n",
    "axs[1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "\n",
    "Segmentation is the process of defining which objects in our image we want to measure. The goal is to compute a mask, a black and white image, that divides the image into objects (white) and background (black). \n",
    "\n",
    "The goal is to find masks which represents each ROI. The easiest way to do this by simply thresholding the intensity of the image. By this, the mask is defined such that only intensity (pixel) values greater than the threshold value are selected. If we stop at this stage, the problem we may encounter is such that some values within the thresholded region may get discarded because the intensity value is lower than the threshold and the ROI won't be filled. In this case, we may need to find the edge or/and fill the mask region. Another issue that we may encounter as well is that the edges of the regions may overlap so they may be detected or segmented as one. One solution in this case is to use what is called watershed segmentation to detect the flow of the intensity and decide from there whether the regions form only one object or different objects. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding \n",
    "\n",
    "The simplest way of doing this is to use a static threshold, where an intensity value is defined as a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "nuclei = image.data[0,0,3,0,:,:] # the object we want to segment and set as markers of our final ROI\n",
    "nuclei_flt = filters.gaussian(nuclei.astype(float), 10) \n",
    "\n",
    "threshold = 5 # change threshold value here to see what happens to the mask that is being displayed below\n",
    "mask = nuclei_flt >= threshold    \n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.title('Thresholded image')\n",
    "plt.axis('off');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the contour of this mask over the image to judge the quality of the segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "\n",
    "contours = measure.find_contours(mask, 0.5)\n",
    "\n",
    "plt.imshow(nuclei)\n",
    "# For each elements of the contours\n",
    "for contour in contours:\n",
    "    plt.plot(contour[:,1], contour[:,0])\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('Contours of the segmented image')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated thresholding segmentation \n",
    "\n",
    "If a static threshold is not suitable, a thresholding algorithm can be used. One example for this is Otsu's method [Otsu's method](https://en.wikipedia.org/wiki/Otsu%27s_method). More thresholding algorithms can be found in the [skimage.filters package](https://scikit-image.org/docs/stable/api/skimage.filters.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "# find the threshold value differentiating the signal from background\n",
    "threshold = filters.threshold_otsu(nuclei_flt) \n",
    "\n",
    "# define the threshold mask and close it\n",
    "nuclei_mask = nuclei_flt > threshold\n",
    "\n",
    "fig, ax = plt.subplots(1,3)\n",
    "\n",
    "ax[0].imshow(nuclei_flt)\n",
    "ax[0].set_title('Thresholded image')\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "ax[1].hist(nuclei.ravel(),log=True)\n",
    "ax[1].plot([threshold, threshold], [0,5e6])\n",
    "ax[1].set_title('Otsu threshold')\n",
    "ax[1].set_aspect(5)\n",
    "\n",
    "ax[2].imshow(nuclei_mask)\n",
    "ax[2].set_title('Thresholded image')\n",
    "ax[2].set_axis_off()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up the binary mask using morphology tools from scikit image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import morphology\n",
    "from skimage import segmentation\n",
    "from skimage import measure\n",
    "from skimage import color\n",
    "\n",
    "# remove artifacts connected to image border\n",
    "only_large_objects = morphology.remove_small_objects(nuclei_mask, 10000)\n",
    "cleared = morphology.closing(only_large_objects, morphology.square(35))\n",
    "cleared = segmentation.clear_border(cleared)\n",
    "\n",
    "# label image regions\n",
    "nuclei_label = measure.label(cleared)\n",
    "\n",
    "# label image to rgb \n",
    "nuclei_label_overlay = color.label2rgb(nuclei_label, image=10*nuclei, bg_label=0)\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(5,5))\n",
    "\n",
    "ax[0,0].imshow(nuclei_flt)\n",
    "ax[0,0].set_title('Filtered Nuclei channel')\n",
    "ax[0,0].set_axis_off()\n",
    "\n",
    "\n",
    "ax[0,1].imshow(nuclei_mask)\n",
    "ax[0,1].set_title('Otsu threshold')\n",
    "ax[0,1].set_axis_off()\n",
    "\n",
    "ax[0,2].imshow(only_large_objects)\n",
    "ax[0,2].set_title('Large Objects')\n",
    "ax[0,2].set_axis_off()\n",
    "\n",
    "ax[1,0].imshow(cleared)\n",
    "ax[1,0].set_title('Cleared mask')\n",
    "ax[1,0].set_axis_off()\n",
    "\n",
    "ax[1,1].imshow(nuclei_label)\n",
    "ax[1,1].set_title('Label image')\n",
    "ax[1,1].set_axis_off()\n",
    "\n",
    "ax[1,2].imshow(nuclei_label_overlay)\n",
    "ax[1,2].set_title('Mask and image overlay')\n",
    "ax[1,2].set_axis_off()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watershed-based segmentation\n",
    "The segmentation of the nuclei is pretty straightforward because they are well separated. In same cases however, the objects we want to segment overlap, making it difficult to discern the different regions. In this case, we would need to add extra step into the image segmentation in order to discern the objects. Watershed segmentation is useful for this purpose. It is generally used for separating different objects. Watershed algorithm treats pixels values as a local topography (elevation) so the goal is to follow the flow of the elevation and find the watershed lines from which the edge of each of the regions are deduced. We use this principle to extract our previously defined ROIs: the actin region in Channel-1 having the two nuclei as markers and the seeds of the segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actin  = image.data[0,0,1,0,:,:] #  the channel we want to segment \n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].imshow(actin)\n",
    "axs[0].set_title('Actin channel')\n",
    "axs[0].set_axis_off()\n",
    "\n",
    "axs[1].imshow(nuclei)\n",
    "axs[1].set_title('Nuclei channel')\n",
    "axs[1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask in the actin channel\n",
    "Create a mask on the actin using thresholding and morphological operations for closing and filling the mask. In the morphological operations, the erosion and dilation operations are used. Erosion removes small objects so only the big objects will remain in the image while dilation makes small objects more visible and fills in small holes in objects. A successive operation of those two operations dilation followed by an erosion constitutes the closing function.\n",
    "\n",
    "The dynamic range of the intensity values of the actin image is low. The image also has a 8 bit depth. Because of these, we needed to do a lot of filtering to smooth the image and still did not obtain a good mask which can accurately covers the actin (see the figure representing the filled and closed mask above). A solution to this can consist of increasing the bit depth, hence the dynamic range, and blurring the image using a Gaussian filter afterwards. We convert the 8-bit image into 32-bit floating point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 8-bit into 32-bit floating point\n",
    "print('(min, max) of the dynamic range =', (np.min(actin), np.max(actin)))\n",
    "print('bit depth: ', actin.dtype)\n",
    "\n",
    "actin_float = actin.astype(np.float32)\n",
    "print('bit depth: ', actin_float.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage as ndi\n",
    "\n",
    "# smoothed the image using the Gaussian filter with the same sigma as before\n",
    "smooth_actin = filters.gaussian(actin_float, sigma=3)\n",
    "\n",
    "# find the optimal threshold\n",
    "threshold = 0.5\n",
    "\n",
    "# define the threshold mask\n",
    "thresh_mask = smooth_actin > threshold\n",
    "\n",
    "# fill holes to form the the actin_mask without performing the closing function\n",
    "actin_mask = ndi.binary_fill_holes(thresh_mask).astype(int)\n",
    "\n",
    "fig, axs = plt.subplots(1,4, figsize=(15, 15))\n",
    "axs[0].imshow(actin)\n",
    "axs[0].set_title('Original image')\n",
    "axs[0].set_axis_off()\n",
    "\n",
    "axs[1].imshow(smooth_actin)\n",
    "axs[1].set_title('Gaussian filter')\n",
    "axs[1].set_axis_off()\n",
    "\n",
    "axs[2].imshow(thresh_mask)\n",
    "axs[2].set_title('Thresholded image')\n",
    "axs[2].set_axis_off()\n",
    "\n",
    "axs[3].imshow(actin_mask)\n",
    "axs[3].set_title('Fill holes')\n",
    "axs[3].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance transform and watershed\n",
    "The next step will consists of computing the distance transform of the actin mask. Observe how the value of the distance inside the regions change with the spatial coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = ndi.distance_transform_edt(actin_mask) \n",
    "\n",
    "fig, ax = plt.subplots(1,3)\n",
    "\n",
    "ax[0].imshow(actin_mask)\n",
    "ax[0].set_title('Mask')\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "im = ax[1].imshow(distance, cmap='jet')\n",
    "ax[1].set_title('Distance transform')\n",
    "ax[1].set_axis_off()\n",
    "\n",
    "# insert the colorbar \n",
    "ax[2].set_position([0.64,0.4,0.025,0.2])\n",
    "fig.colorbar(im, cax=ax[2]);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set the nuclei we segmented before as the markers or seeds of the segmentations. We use the markers to label the segmented regions in the actin region. \n",
    "After, we watershed the negative inverse of the distance transform of the actin using the defined markers and within the mask delimited by the actin. The negative inverse of the distance is used in the watershed function instead of the distance because the goal is to have the objects region as valleys not peak.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_label = segmentation.watershed(-distance, markers = nuclei_label, mask = actin_mask)\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=(15, 15))\n",
    "\n",
    "# create a RGB composite image to display the two channels (normalize channel in between 0 and 1)\n",
    "rgb = np.stack([actin.astype(float) / actin.max(), \n",
    "                np.zeros(actin.shape), \n",
    "                nuclei.astype(float) / nuclei.max()], axis=2)\n",
    "\n",
    "ax[0].imshow(rgb)\n",
    "ax[0].set_axis_off()\n",
    "ax[0].set_title('Actin and nuclei')\n",
    "\n",
    "ax[1].imshow(distance, cmap='jet')\n",
    "ax[1].set_title('Distance')\n",
    "ax[1].set_axis_off()\n",
    "\n",
    "ax[2].imshow(nuclei_label)\n",
    "ax[2].set_title('Markers (seeds of segmentation)')\n",
    "ax[2].set_axis_off()\n",
    "\n",
    "ax[3].imshow(cell_label)\n",
    "ax[3].set_title('Segmented ROIs')\n",
    "ax[3].set_axis_off()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation using Cellpose\n",
    "\n",
    "Use a pretrained neural network for segmenting the image using two channels. [Cellpose](https://cellpose.readthedocs.io/en/latest/) has been pretrained to detect cells or nuclei on a number of biological images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from cellpose import models\n",
    "\n",
    "# Get the two channels from the data\n",
    "data = image.data[0,[1,3],0,::8,::8].squeeze()\n",
    "\n",
    "# Get the pretrained model\n",
    "model = models.Cellpose(gpu=False, model_type='cyto2')\n",
    "\n",
    "# Evaluate the model\n",
    "mask, flows, styles, diams = model.eval(data, channels=[1,2], diameter=100, cellprob_threshold=2)\n",
    "\n",
    "\n",
    "# Display the results\n",
    "fig, ax = plt.subplots(1,4)\n",
    "ax[0].imshow(rgb)\n",
    "ax[0].set_axis_off()\n",
    "ax[1].imshow(mask)\n",
    "ax[1].set_axis_off()\n",
    "ax[1].set_title('Labels')\n",
    "ax[2].imshow(flows[0])\n",
    "ax[2].set_axis_off()\n",
    "ax[2].set_title('Flows')\n",
    "ax[3].imshow(flows[2])\n",
    "ax[3].set_axis_off()\n",
    "ax[3].set_title('Probability')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot detection\n",
    "For spot detection, we need to use what is called blob detection, which aims at detecting regions in a digital image that differ in properties, such as brightness or color, compared to surrounding regions. The Difference of Gaussian (DoG) method is one technique to detect blobs. For each blob found (a spot in our case), the method returns its coordinates position and the standard deviation of the Gaussian kernel that detected the spot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature \n",
    "import matplotlib.collections as mc\n",
    "\n",
    "punctate = image.data[0,2,:,:].squeeze() # the image containing the punctate structure we want to detect\n",
    "\n",
    "# Difference of Gaussian (DOG) return a (N,3) array with (row, columns and sigma)\n",
    "coordinates = feature.blob_dog(punctate, max_sigma=10, threshold=0.01) \n",
    "\n",
    "# Visualization of the result\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "# Display the original image\n",
    "ax[0].imshow(punctate,cmap='gray')\n",
    "ax[0].set_title('Punctate')\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "# Display the original image with an overlay of the detected spots\n",
    "ax[1].imshow(punctate, cmap='gray') \n",
    "ax[1].add_collection(mc.CircleCollection(\n",
    "        sizes = 5 * coordinates[:,2], offsets = coordinates[:,1::-1], \n",
    "        transOffset = ax[1].transData, color='red', facecolor='none'))\n",
    "    \n",
    "ax[1].set_axis_off()\n",
    "ax[1].set_title('Detected punctates using DoG')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region properties\n",
    "\n",
    "Now let use the ROI masks that we have just segmented and do the measurement we previously defined as our problems. For this, we use the regionprops function to measure the properties of the signal within the ROI mask. Below we display the countours of the ROIs.\n",
    "\n",
    "## Maximum Intensity\n",
    "\n",
    "The regionprops function is crucial for this step. It allows you to define a list of geometrical and intensity properties which can be measured in your images. \n",
    "\n",
    "This is a selection of useful properties supported by regionprops:\n",
    "  - area\n",
    "  - bbox\n",
    "  - bbox_area\n",
    "  - centroid\n",
    "  - coords\n",
    "  - equivalent_diameter\n",
    "  - label\n",
    "  - major_axis_length\n",
    "  - max_intensity\n",
    "  - mean_intensity\n",
    "  - min_intensity\n",
    "  - minor_axis_length\n",
    "  - slice\n",
    "  - solidity\n",
    "  - eccentricity\n",
    "  - orientation\n",
    "  - perimeter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "signal = image.data[0,0].squeeze()\n",
    "signal_table = measure.regionprops_table(cell_label, signal, properties=('label', 'intensity_max'))\n",
    "results = pd.DataFrame(signal_table)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "signal = image.data[0,0].squeeze()\n",
    "\n",
    "signal_props = measure.regionprops(cell_label, signal)\n",
    "actin_props = measure.regionprops(cell_label, actin)\n",
    "\n",
    "# Get the list of labels from the regions properties\n",
    "labels = [p.label for p in signal_props]\n",
    "\n",
    "# Get the maximum intensity for the properties\n",
    "signal_max_intensity = [p.intensity_max for p in signal_props]\n",
    "actin_max_intensity = [p.intensity_max for p in actin_props]\n",
    "\n",
    "# Gather the results in a data frame (table)\n",
    "results = pd.DataFrame({\n",
    "    'Region label': labels, \n",
    "    'Signal [ch0] maximum intensity': signal_max_intensity,\n",
    "    'Actin [ch1] maximum intensity': actin_max_intensity})\n",
    "\n",
    "results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a custom measurement\n",
    "\n",
    "The information about the median is currently not included in the regionprops function. Here we show how can we extract the intensity median for each ROI using a custom function. The median of a list is defined as the middle element of the list when the list is sorted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to measure median intensity\n",
    "def median_intensity(mask, intensity):\n",
    "    '''Median intenisty in a mask\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mask : binary image\n",
    "    intensity: intensity  image\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The median intensity in the mask\n",
    "    '''\n",
    "    return np.median(intensity[mask])\n",
    "\n",
    "actin_props = measure.regionprops_table(cell_label, actin, \n",
    "                                 properties = ('label','mean_intensity'), \n",
    "                                 extra_properties = (median_intensity,))\n",
    "\n",
    "results = pd.DataFrame(actin_props)\n",
    "\n",
    "results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of dots\n",
    "Now we are going to count how many dots are there in Channel-2 within the two regions defined by the ROIs. We use the knowledge we have acquired from Section. Spot detection to complete this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "props = measure.regionprops(cell_label)\n",
    "\n",
    "# Get the labels at the coordinates given by the blob detector\n",
    "blob_cell_label = cell_label[coordinates[:,0].astype(int), coordinates[:,1].astype(int)]\n",
    "\n",
    "# Get the list of labels from the regions properties\n",
    "labels = [p.label for p in props]\n",
    "\n",
    "# Compute the number of blob for each label\n",
    "cell_num_blob = [np.sum(blob_cell_label == label)  for label in labels]\n",
    "\n",
    "# Gather the results in a data frame (table)\n",
    "results = pd.DataFrame({\n",
    "    'Region label': labels, \n",
    "    'Number of punctate': cell_num_blob })\n",
    "\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative approach using extra_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a function to count the number of punctate in a region\n",
    "def num_punctate(mask:np.ndarray, intensity:np.ndarray) -> int:\n",
    "    '''Number of punctate in the mask\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mask      : binary image for the region\n",
    "    intensity :  binary image indicating the presence of a blob\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The number of puncates in the region\n",
    "    '''\n",
    "    return int(intensity[mask].sum())\n",
    "\n",
    "# Create a mask for the blobs\n",
    "blob_mask = np.zeros(cell_label.shape)\n",
    "blob_mask[coordinates[:,0].astype(int), coordinates[:,1].astype(int)] = 1\n",
    "\n",
    "props = measure.regionprops_table(cell_label, blob_mask, \n",
    "                          properties = ('label',),\n",
    "                          extra_properties = (num_punctate, ))\n",
    "\n",
    "results = pd.DataFrame(props)\n",
    "\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also rename the columns to be more descriptive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.rename(columns = {'label':'Region label', 'num_punctate':'Number of punctate'})\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting tabular results\n",
    "\n",
    "To save results in a csv file use the export function from pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('results.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more helpful tips\n",
    "\n",
    "- scientific images are data, they can be compromised by inappropriate manipulations\n",
    "- take good images:\n",
    "    - don't oversaturate your data\n",
    "    - use the full dynamic range when taking your images\n",
    "- do not segment on the data you are measuring, use a housekeeping channel\n",
    "- images that are compared to each other need to be processed and acquired in the same manner\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonCourse3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
