{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td><p style=\"font-size:45px; color: #55BBD2\">Analysis of light microscopy images in Python</p></td>\n",
    "    <td><img src=\"ressources/lmb_logo.svg\" alt=\"LMB Logo\" width=\"500\" height=\"600\" align=\"right\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><p style=\"font-size:15px; color: #55BBD2\">Version: July 2023</p></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Short remarks\n",
    "short intro to the aim of the course\n",
    "short Intro to Python,  Python packages, installing and packages\n",
    "how to run a jupyter notebook\n",
    "download images\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first download some data. \n",
    "Please run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import os \n",
    "\n",
    "def download_and_unzip(url, extract_to='.'):\n",
    "    http_response = urlopen(url)\n",
    "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
    "    zipfile.extractall(path=extract_to)\n",
    "\n",
    "url = 'https://cloud.mrc-lmb.cam.ac.uk/s/apiQPm28YLRWXeo/download/data.zip'\n",
    "data_folder = os.path.expanduser('~/Python-course')\n",
    "download_and_unzip(url, data_folder)\n",
    "\n",
    "from pathlib import Path\n",
    "print(f'List of files in the {data_folder}data folder')\n",
    "for x in (Path(data_folder) / 'data').glob('*'):\n",
    "    print(f' {str(x.name)}')\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis problem\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](ressources/data.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let say we are interested in what is happening in each channel within the actin region (Channel-1) that surround each nuclei (Channel-3). Our region of interest (ROI) is therefore defined by the actins in Channel-1 that have as a \"seed\" the nuclei in Channel-3. We want to measure:\n",
    "- the maximal intensity in Channel-0 within this ROI,\n",
    "- the integrated intensity in Channel-1 within this ROI,\n",
    "- the median intensity of Channel-2 of the ROI within this ROI  \n",
    "\n",
    "That is enough problem for now so let's get into it :)\n",
    "\n",
    "To solve the problem and achieve the statistic analysis, we generally need to do the following steps:\n",
    "- Load the image\n",
    "- Filter the image\n",
    "- Segment the image\n",
    "- Measure image properties\n",
    "- Save the data\n",
    "\n",
    "\n",
    " \n",
    "This notebook shows how can we extract statistical information from an image (e.g. finding the maximal intensity in a desired region of interest). The steps that we need to follow to achieve this mainly depend on the exact problem we want to solve but in general it consists of: \n",
    "\n",
    "    Step 1: load and read the image (load-data.ipynb)\n",
    "    Step 2: pre-process the image if necessary such as noise filtering (filter-image.ipynb)\n",
    "    Step 3: find the region of interest: edge or inside the region? \n",
    "    Step 4: extract the desired information in the region of interest: mean, max, min, sum, median, mode, ...\n",
    "    Step 5: display/plot results\n",
    "    Step 6: save the result in a given image format or values in a text/excel file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading microscopy image data\n",
    "\n",
    "- How to load an image explain that there are different packages for different image types\n",
    "- We use aicsimageio - Why?, Depending on your image format you can use a different package\n",
    "- explain bioformats\n",
    "\n",
    "When opening an image:\n",
    "- You need to specify the path of the image\n",
    "- Make sure to have the path correct and on MS Windows use / instead of \\\n",
    "\n",
    "Load the image with its metadata in this next cell by replacing the image path directory in that cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aicsimageio import AICSImage\n",
    "\n",
    "image = AICSImage('data/tetrahymena.nd2')\n",
    "\n",
    "image.data\n",
    "image.metadata\n",
    "image.physical_pixel_sizes\n",
    "image.shape\n",
    "image.dims\n",
    "image.channel_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image and its metadata are now loaded into the variable \"mydata\".\n",
    "\n",
    "\n",
    "\n",
    "What is metadata.\n",
    "Next, we read the image and the metadata. Metadata includes the number of channels in the images, the name of each channel and emission wavelength associated with each channel, the voxel size and physical size of the image. Many other acquisition parameters are stored in the metadata and we can access them from here.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data we'll use along this lession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = AICSImage('data/airyscan-4colors.tif')\n",
    "\n",
    "for k, i in enumerate(image.dims.order):\n",
    "    print(f'Array axis {k} is {i} with size {image.dims[i][0]}')\n",
    "\n",
    "print(f'The name of the channels are {image.channel_names}')\n",
    "print(f'The pixel size in X is {image.physical_pixel_sizes.X:.4f} microns')\n",
    "print(f'The pixel size in Y is {image.physical_pixel_sizes.Y:.4f} microns')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now lets extract image data and look at it.\n",
    "- what is an image?\n",
    "- explain image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the numpy package used for manipulating images\n",
    "import numpy as np\n",
    "\n",
    "# get the array of pixels intensity as a numpy object\n",
    "data = image.data\n",
    "\n",
    "# Print the type of the array\n",
    "print('The data is a ', type(data)) \n",
    "\n",
    "# Print the dimension of the array\n",
    "print('The array has the following shape', data.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib for loading images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's display a 2D image in a figure\n",
    "plt.imshow(data[0,1,0,:,:], cmap='gray');\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display each channel of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotli for displaying images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# compute the number of channels\n",
    "num_channels = image.shape[1]\n",
    "\n",
    "# Display the image for each channel\n",
    "fig, ax = plt.subplots(1, num_channels, figsize=(16,4))\n",
    "for k  in range(num_channels):    \n",
    "    ax[k].imshow(image.data[0,k,0], cmap='hot')\n",
    "    ax[k].set_axis_off()\n",
    "    ax[k].set_title(image.channel_names[k])    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image is a multi-dimensional array of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop and downsample the array by a factor of 5\n",
    "crop = image.data[0, 0, 0, 850:950:5, 900:1000:5]\n",
    "\n",
    "# Create a figure with axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the downsampled array in the figure\n",
    "plt.imshow(crop, cmap='gray')\n",
    "\n",
    "# Add the values of the pixel intensity\n",
    "for i in range(crop.shape[0]):\n",
    "    for j in range(crop.shape[1]):\n",
    "        c = 'white' if crop[i, j] < 5 else 'black'\n",
    "        ax.text(j, i, str(int(crop[i, j])), color=c, ha='center', va='center', size=10)\n",
    "ax.axis(\"off\")\n",
    "plt.title('Pixel values');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Filtering and Noise\n",
    "\n",
    "Noise in microscopic image is unavoidable during image acquisition. There are three main categories of noise: \n",
    "1)Photon noise, also called fluorescence noise or shot noise: acquired due to the fluctuation of photon emission to the detector. This is the most dominant noise. It is signal dependent and follows a Poisson distribution. \n",
    "2) Electronic noise, also called dark current or dark noise: accumulated over time due to thermal change as the detector heats up. It is unavoidable but negligible compared to photon noise and is not photon dependent. It follows Poisson distribution. \n",
    "3) Readout noise: digital noise acquired during the electronic quantisation of photons. It follows a Gaussian distribution where the standard deviation is constant and the mean is 0.\n",
    "\n",
    "Dominant noise in fluorescence microscopy follows either a Gaussian distribution or Poisson distribution or both. Gaussian noise is additive while Poisson noise is signal dependent. A correct removal of those two different types of noise may therefore differ. Firstly, we will look at how those noise affect the image differently. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating noisy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a noise free test image\n",
    "n = 100\n",
    "nphotons = 5\n",
    "data = np.zeros((n,n),dtype=float)+0.5\n",
    "data[n//4-10:n//4+10, n//4-10:n//4+10] = nphotons\n",
    "data[n//4-10:n//4+10, 3*n//4-10:3*n//4+10] = nphotons\n",
    "data[3*n//4-10:3*n//4+10, n//4:3*n//4] = nphotons\n",
    "\n",
    "# Add a Gaussian noise\n",
    "mean = 0\n",
    "stddev = 1\n",
    "noisy_gauss_img = data + np.random.normal(mean, stddev, data.shape)\n",
    "\n",
    "# Apply Poisson noise\n",
    "noisy_poisson_img = np.random.poisson(data)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "axs[0].imshow(data, cmap='gray',vmin=-3,vmax=nphotons+3)\n",
    "axs[0].set_title('Original image')\n",
    "axs[0].set_axis_off()\n",
    "\n",
    "axs[1].imshow(noisy_gauss_img, cmap='gray',vmin=-3,vmax=nphotons+3)\n",
    "axs[1].set_title('With Gaussian noise')\n",
    "axs[1].set_axis_off()\n",
    "\n",
    "axs[2].imshow(noisy_poisson_img, cmap='gray',vmin=-3,vmax=nphotons+3)\n",
    "axs[2].set_title('With Poisson noise')\n",
    "axs[2].set_axis_off()\n",
    "\n",
    "fig.set_tight_layout('tight')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise filtering\n",
    "\n",
    "There are different types of noise reduction: smoothing filtering or convolution filtering, mean filtering, median filtering, and frequency filtering. An image can be treated as a matrix, or array in Python. Each pixel has a value which represents the image intensity at the spatial position of the pixel. To filter an image, we need to work on each pixel of the image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian filter\n",
    "A smoothing filter uses the principle of convolution to reduce the noise. A convolution process is similar to drawing. The kernel is the pencil that is used for the drawing. So the sharpness of the drawn image really depends on the width of the point of the pencil. In the case of a Gaussian filtering, the Gaussian distribution is used as a kernel. When used for noise filterning, the noise which should have a lower intensity value than the real signal gets smoothed out because of this convolution process. Next, we will see below this process work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "# Crop the image and select the first channel\n",
    "crop = image.data[0, 0, 0, 850:950, 900:1000]\n",
    "\n",
    "# Gaussian filtering of data with a Gaussian filter of scale 3\n",
    "gaussian_filtered = filters.gaussian(crop.astype(float), 3)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "vmin, vmax = crop.min(), crop.max()\n",
    "\n",
    "axs[0].imshow(crop, vmin = vmin, vmax = vmax)                      # original image\n",
    "axs[0].set_title('Noisy image')\n",
    "axs[0].set_axis_off()\n",
    "\n",
    "axs[1].imshow(gaussian_filtered,vmin=vmin,vmax=vmax)                # with addition Gaussian noise\n",
    "axs[1].set_title('Gaussian filtered ($\\sigma=5$)')\n",
    "axs[1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median filter\n",
    "Different techniques for noise filtering are median and total variation (TV) filtering. With the median filter, each output pixel is computed as the median value of the input pixel under a chosen window. With TV filtering technique, the output image is an approximation of the input noisy image, which has a smaller total variation than the input image but is similar to the image. A total variation of an image measures how much the image changes between pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The second argument in this function here defines the size of the kernel over \n",
    "# which the median is calculated\n",
    "median_filtered = filters.median(crop, np.ones((5,5))) \n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "# compute the min and max of the original image to keep the same display range\n",
    "vmin, vmax = crop.min(), crop.max()\n",
    "\n",
    "axs[0].imshow(crop, vmin = vmin, vmax = vmax)    \n",
    "axs[0].set_title('Noisy image')\n",
    "axs[0].set_axis_off()\n",
    "\n",
    "axs[1].imshow(median_filtered,  vmin = vmin, vmax = vmax)    \n",
    "axs[1].set_title('Median filtered (5x5)')\n",
    "axs[1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "\n",
    "- what is segmentation\n",
    "- what is a mask , label mask, ROI\n",
    "\n",
    "The goal is to find masks which represents each ROI. The easiest way to do this by simply thresholding the intensity of the image. By this, the mask is defined such that only intensity (pixel) values greater than the threshold value are selected. If we stop at this stage, the problem we may encounter is such that some values within the thresholded region may get discarded because the intensity value is lower than the threshold and the ROI won't be filled. In this case, we may need to find the edge or/and fill the mask region. Another issue that we may encounter as well is that the edges of the regions may overlap so they may be detected or segmented as one. One solution in this case is to use what is called watershed segmentation to detect the flow of the intensity and decide from there whether the regions form only one object or different objects. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclei = image.data[0,3,0,:,:] # the object we want to segment and set as markers of our final ROI\n",
    "nuclei_flt = filters.gaussian(nuclei.astype(float), 10) \n",
    "\n",
    "threshold = 5 # change threshold value here to see what happens to the mask that is being displayed below\n",
    "mask = nuclei_flt >= threshold    \n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.title('Thresholded image')\n",
    "plt.axis('off');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated thresholding segmentation \n",
    "There is also one way to automatically detect the threshold that needs to be applied on the image in order to detect the nuclei region. This method was developed by Otsu and you can follow the link [here](https://en.wikipedia.org/wiki/Otsu%27s_method) if you want to learn more how does it work. So let us see now how can we do the thresholding using the Otsu's method and fill the mask. And since in microscopic images, we may have more than one nuleus (in our case we do have 2 full ones), we need to label each of the nuclei to distingush them and to be able to extract information from each of them. To achieve these, we need to load some packages first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "# find the threshold value differentiating the signal from background\n",
    "threshold = filters.threshold_otsu(nuclei_flt) \n",
    "\n",
    "# define the threshold mask and close it\n",
    "nuclei_mask = nuclei_flt > threshold\n",
    "\n",
    "fig, ax = plt.subplots(1,3)\n",
    "\n",
    "ax[0].imshow(nuclei_flt)\n",
    "ax[0].set_title('Thresholded image')\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "ax[1].hist(nuclei.ravel(),log=True)\n",
    "ax[1].plot([threshold, threshold], [0,5e6])\n",
    "ax[1].set_title('Otsu threshold')\n",
    "ax[1].set_aspect(5)\n",
    "\n",
    "ax[2].imshow(nuclei_mask)\n",
    "ax[2].set_title('Thresholded image')\n",
    "ax[2].set_axis_off()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up the binary mask using morphology tools from scikit image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import morphology\n",
    "from skimage import segmentation\n",
    "from skimage import measure\n",
    "from skimage import color\n",
    "\n",
    "# remove artifacts connected to image border\n",
    "cleared = morphology.closing(nuclei_mask, morphology.square(35))\n",
    "cleared = segmentation.clear_border(cleared)\n",
    "\n",
    "# label image regions\n",
    "nuclei_label = measure.label(cleared)\n",
    "\n",
    "# label image to rgb \n",
    "nuclei_label_overlay = color.label2rgb(nuclei_label, image=10*nuclei, bg_label=0)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(5,5))\n",
    "ax[0,0].imshow(nuclei_mask)\n",
    "ax[0,0].set_title('Ostu threshold')\n",
    "ax[0,0].set_axis_off()\n",
    "\n",
    "ax[0,1].imshow(cleared)\n",
    "ax[0,1].set_title('Cleared mask')\n",
    "ax[0,1].set_axis_off()\n",
    "\n",
    "ax[1,0].imshow(nuclei_label)\n",
    "ax[1,0].set_title('Label image')\n",
    "ax[1,0].set_axis_off()\n",
    "\n",
    "ax[1,1].imshow(nuclei_label_overlay)\n",
    "ax[1,1].set_title('Mask and image overlay')\n",
    "ax[1,1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watershed-based segmentation\n",
    "The segmentation of the nuclei is pretty straightforward because they are well separated. In same cases however, the objects we want to segment overlap, making it difficult to discern the different regions. In this case, we would need to add extra step into the image segmentation in order to discern the objects. Watershed segmentation is useful for this purpose. It is generally used for separating different objects. Watershed algorithm treats pixels values as a local topography (elevation) so the goal is to follow the flow of the elevation and find the watershed lines from which the edge of each of the regions are deduced. We use this principle to extract our previously defined ROIs: the actin region in Channel-1 having the two nuclei as markers and the seeds of the segmentation. \n",
    "\n",
    "The steps that we need to follow for this are as follows: \n",
    "*  Step 1: threshold the actin to segment to obtain a region mask denoted by actin_mask from it\n",
    "*  Step 2: calculate the distance transform of actin_mask, also known as distance map, which defines the spatial distance of each pixel value position to the background of the image\n",
    "*  Step 3: define the markers or seeds of the segmentation as local maxima of the distance to the background (local minima of the image to segment and the opposite of the distance). We have this as the nuclei_label_image. \n",
    "*  Step 4: watershed the opposite of the distance transform from Step 2 within the mask in Step 1 and the nuclei markers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actin  = image.data[0,1,0,:,:] #  the channel we want to segment \n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].imshow(actin)\n",
    "axs[0].set_title('Actin channel')\n",
    "axs[0].set_axis_off()\n",
    "\n",
    "axs[1].imshow(nuclei)\n",
    "axs[1].set_title('Nuclei channel')\n",
    "axs[1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask in the actin channel\n",
    "Create a mask on the actin using thresholding and morphological operations for closing and filling the mask. In the morphological operations, the erosion and dilation operations are used. Erosion removes small objects so only the big objects will remain in the image while dilation makes small objects more visible and fills in small holes in objects. A successive operation of those two operations dilation followed by an erosion constitutes the closing function.\n",
    "\n",
    "The dynamic range of the intensity values of the actin image is low. The image also has a 8 bit depth. Because of these, we needed to do a lot of filtering to smooth the image and still did not obtain a good mask which can accurately covers the actins (see the figure representing the filled and closed mask above). A solution to this can consist of increasing the bit depth, hence the dynamic range, and blurring the image using a Gaussian filter afterwards. We convert the 8-bit image into 32-bit floating point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 8-bit into 32-bit floating point\n",
    "print('(min, max) of the dynamic range =', (np.min(actin), np.max(actin)))\n",
    "print('bit depth: ', actin.dtype)\n",
    "\n",
    "actin_float = actin.astype(np.float32)\n",
    "print('bit depth: ', actin_float.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage as ndi\n",
    "\n",
    "# smoothed the image using the Gaussian filter with the same sigma as before\n",
    "smooth_actin = filters.gaussian(actin_float, sigma=3)\n",
    "\n",
    "# find the optimal threshold\n",
    "threshold = 0.5\n",
    "\n",
    "# define the threshold mask\n",
    "thresh_mask = smooth_actin > threshold\n",
    "\n",
    "# fill holes to form the the actin_mask without performing the closing function\n",
    "actin_mask = ndi.binary_fill_holes(thresh_mask).astype(int)\n",
    "\n",
    "fig, axs = plt.subplots(1,4, figsize=(15, 15))\n",
    "axs[0].imshow(actin)\n",
    "axs[0].set_title('Original image')\n",
    "axs[0].set_axis_off()\n",
    "\n",
    "axs[1].imshow(smooth_actin)\n",
    "axs[1].set_title('Gaussian filter')\n",
    "axs[1].set_axis_off()\n",
    "\n",
    "axs[2].imshow(thresh_mask)\n",
    "axs[2].set_title('Thresholded image')\n",
    "axs[2].set_axis_off()\n",
    "\n",
    "axs[3].imshow(actin_mask)\n",
    "axs[3].set_title('Fill holes')\n",
    "axs[3].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance transform and watershed\n",
    "The next step will consists of computing the distance transform of the actin mask. An an exercise, observe how the value of the distance inside the regions change with the spatial coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = ndi.distance_transform_edt(actin_mask) \n",
    "\n",
    "fig, ax = plt.subplots(1,3)\n",
    "\n",
    "ax[0].imshow(actin_mask)\n",
    "ax[0].set_title('Mask')\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "im = ax[1].imshow(distance, cmap='jet')\n",
    "ax[1].set_title('Distance transform')\n",
    "ax[1].set_axis_off()\n",
    "\n",
    "# insert the colorbar \n",
    "ax[2].set_position([0.64,0.4,0.025,0.2])\n",
    "fig.colorbar(im, cax=ax[2]);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set the nuclei we segemented before as the markers or seeds of the segmentations. We use the markers to label the segmented regions in the actin region. \n",
    "After, we watershed the negative inverse of the distance transform of the actin using the defined markers and within the mask delimited by the actin. The negative inverse of the distance is used in the watershed function instead of the distance because the goal is to have the objects region as valleys not peak.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_label = segmentation.watershed(-distance, markers = nuclei_label, mask = actin_mask)\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=(15, 15))\n",
    "\n",
    "# create a RGB composite image to display the two channels (normalize channel in between 0 and 1)\n",
    "rgb = np.stack([actin.astype(float) / actin.max(), \n",
    "                np.zeros(actin.shape), \n",
    "                nuclei.astype(float) / nuclei.max()], axis=2)\n",
    "\n",
    "ax[0].imshow(rgb)\n",
    "ax[0].set_axis_off()\n",
    "ax[0].set_title('Actin and nuclei')\n",
    "\n",
    "ax[1].imshow(distance, cmap='jet')\n",
    "ax[1].set_title('Distance')\n",
    "ax[1].set_axis_off()\n",
    "\n",
    "ax[2].imshow(nuclei_label)\n",
    "ax[2].set_title('Markers (seeds of segmentation)')\n",
    "ax[2].set_axis_off()\n",
    "\n",
    "ax[3].imshow(cell_label)\n",
    "ax[3].set_title('Segmented ROIs')\n",
    "ax[3].set_axis_off()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation using Cellpose\n",
    "\n",
    "Use a pretrained neural network for segmenting the image using two channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellpose import models\n",
    "\n",
    "# Get the two channels from the data\n",
    "data = image.data[0,[1,3],0,::10,::10].squeeze() \n",
    "\n",
    "# Get the pretrained model\n",
    "model = models.Cellpose(gpu=False, model_type='cyto2')\n",
    "\n",
    "# Evaluate the model \n",
    "mask, flows, styles, diams = model.eval(data, channels=[1,2], diameter=200)\n",
    "\n",
    "# Display the results\n",
    "fig, ax = plt.subplots(1,4)\n",
    "ax[0].imshow(rgb)\n",
    "ax[1].set_axis_off()\n",
    "ax[1].imshow(mask)\n",
    "ax[1].set_axis_off()\n",
    "ax[1].set_title('Labels')\n",
    "ax[2].imshow(flows[0])\n",
    "ax[2].set_axis_off()\n",
    "ax[2].set_title('Flows')\n",
    "ax[3].imshow(flows[2])\n",
    "ax[3].set_axis_off()\n",
    "ax[3].set_title('Probability')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot detection\n",
    "\n",
    "To do the spot detection, we need to use what is called blob detection. Blob stands for Binary Large Object and refers to the connected pixel in the binary image. The blob detection method aims at detecting regions in a digital image that differ in properties, such as brightness or color, compared to surrounding regions. The Difference of Gaussian (DoG) method is one technique to detect blobs. For each blob found (a spot in our case), the method returns its coordinates position and the standard deviation of the Gaussian kernel that detected the spot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature \n",
    "import matplotlib.collections as mc\n",
    "\n",
    "punctate = image.data[0,2,:,:].squeeze() # the image containing the punctate structure we want to detect\n",
    "\n",
    "# Difference of Gaussian (DOG) return a (N,3) array with (row, columns and sigma)\n",
    "coordinates = feature.blob_dog(punctate, max_sigma=10, threshold=0.01) \n",
    "\n",
    "# Visualization of the result\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "# Display the original image\n",
    "ax[0].imshow(punctate,cmap='gray')\n",
    "ax[0].set_title('Punctate')\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "# Display the original image with an overlay of the detected spots\n",
    "ax[1].imshow(punctate, cmap='gray') \n",
    "ax[1].add_collection(mc.CircleCollection(\n",
    "        sizes = 5 * coordinates[:,2], offsets = coordinates[:,1::-1], \n",
    "        transOffset = ax[1].transData, color='red', facecolor='none'))\n",
    "    \n",
    "ax[1].set_axis_off()\n",
    "ax[1].set_title('Detected punctates using DoG')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region properties\n",
    "\n",
    "Now let use the ROI masks that we have just segmented and do the measurement we previously defined as our problems. For this, we use the regionprops function to measure the properties of the signal within the ROI mask. Below we display the countours of the ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "\n",
    "props = measure.regionprops(cell_label) # signal property: first argument is the mask ROI, second argument is the signal from which we are to extract any information of interest from\n",
    "\n",
    "# Get a default list of colora\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "# Display the RGB composite image\n",
    "plt.imshow(rgb)\n",
    "\n",
    "# For earch region \n",
    "for k, p in enumerate(props):\n",
    "    # Compute the contours\n",
    "    contours = measure.find_contours(cell_label == p.label, 0.5)\n",
    "    # For each elements of the contours\n",
    "    for contour in contours:\n",
    "        # Draw the contours \n",
    "        plt.plot(contour[:,1], contour[:,0], color=colors[k])\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('Contours of the segmented image');\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Intensity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "signal = image.data[0,0].squeeze()\n",
    "\n",
    "signal_table = measure.regionprops_table(cell_label, signal, properties=('label', 'intensity_max'))\n",
    "\n",
    "results = pd.DataFrame(signal_table)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "signal = image.data[0,0].squeeze()\n",
    "\n",
    "signal_props = measure.regionprops(cell_label, signal)\n",
    "actin_props = measure.regionprops(cell_label, actin)\n",
    "\n",
    "# Get the list of labels from the regions properties\n",
    "labels = [p.label for p in signal_props]\n",
    "\n",
    "# Get the maximum intensity for the properties\n",
    "signal_max_intensity = [p.intensity_max for p in signal_props]\n",
    "actin_max_intensity = [p.intensity_max for p in actin_props]\n",
    "\n",
    "# Gather the results in a data frame (table)\n",
    "results = pd.DataFrame({\n",
    "    'Region label': labels, \n",
    "    'Signal [ch0] maximum intensity': signal_max_intensity,\n",
    "    'Actin [ch1] maximum intensity': actin_max_intensity})\n",
    "\n",
    "results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a custom measurement\n",
    "\n",
    "The information about the median is  not included in the image property that we can retrieve from regionprops. So here we show how can we do it using the information we can extract from the regionprops and numpy. The median of a list is defined as the middle element of the list when the list is sorted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to measure median intensity\n",
    "def median_intensity(mask, intensity):\n",
    "    '''Median intenisty in a mask\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mask : binary image\n",
    "    intensity: intensity  image\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The median intensity in the mask\n",
    "    '''\n",
    "    return np.median(intensity[mask])\n",
    "\n",
    "actin_props = measure.regionprops_table(cell_label, actin, \n",
    "                                 properties = ('label','mean_intensity'), \n",
    "                                 extra_properties = (median_intensity,))\n",
    "\n",
    "results = pd.DataFrame(actin_props)\n",
    "\n",
    "results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of dots\n",
    "Now we are going to count how many dots are there in Channel-2 within the two regions defined by the ROIs. We use the knowledge we have acquired from Section. Spot detection to complete this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "props = measure.regionprops(cell_label)\n",
    "\n",
    "# Get the labels at the coordinates given by the blob detector\n",
    "blob_cell_label = cell_label[coordinates[:,0].astype(int), coordinates[:,1].astype(int)]\n",
    "\n",
    "# Get the list of labels from the regions properties\n",
    "labels = [p.label for p in props]\n",
    "\n",
    "# Compute the number of blob for each label\n",
    "cell_num_blob = [np.sum(blob_cell_label == label)  for label in labels]\n",
    "\n",
    "# Gather the results in a data frame (table)\n",
    "results = pd.DataFrame({\n",
    "    'Region label': labels, \n",
    "    'Number of punctate': cell_num_blob })\n",
    "\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative approach using extra_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a function to count the number of punctate in a region\n",
    "def num_punctate(mask:np.ndarray, intensity:np.ndarray) -> int:\n",
    "    '''Number of punctate in the mask\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mask      : binary image for the region\n",
    "    intensity :  binary image indicating the presence of a blob\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The number of puncates in the region\n",
    "    '''\n",
    "    return int(intensity[mask].sum())\n",
    "\n",
    "# Create a mask for the blobs\n",
    "blob_mask = np.zeros(cell_label.shape)\n",
    "blob_mask[coordinates[:,0].astype(int), coordinates[:,1].astype(int)] = 1\n",
    "\n",
    "props = measure.regionprops_table(cell_label, blob_mask, \n",
    "                          properties = ('label',),\n",
    "                          extra_properties = (num_punctate, ))\n",
    "\n",
    "results = pd.DataFrame(props)\n",
    "\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also rename the columns to be more descriptive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.rename(columns = {'label':'Region label', 'num_punctate':'Number of punctate'})\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting tabular results\n",
    "\n",
    "To save results in a csv file use the export function from pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('results.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more helpful tips\n",
    "\n",
    "- Take good images\n",
    "- Housekeeping labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonCourse3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
